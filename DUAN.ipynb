{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms,models\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function, Variable\n",
    "from __future__ import division\n",
    "from torch.utils import model_zoo\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from scipy.misc import imread, imsave, imresize\n",
    "import glob\n",
    "import random\n",
    "from PIL import Image\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorboardX import SummaryWriter\n",
    "from skimage import exposure, img_as_float\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMG_EXTENSIONS = [\n",
    "    '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
    "    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n",
    "]\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "def find_classes(dir):\n",
    "    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "    classes.sort()\n",
    "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "    return classes, class_to_idx\n",
    "\n",
    "\n",
    "def make_dataset(dir, class_to_idx):\n",
    "    images = []\n",
    "    for target in os.listdir(dir):\n",
    "        d = os.path.join(dir, target)\n",
    "        if not os.path.isdir(d):\n",
    "            continue\n",
    "        for root, _, fnames in sorted(os.walk(d)):\n",
    "            for fname in fnames:\n",
    "                if is_image_file(fname):\n",
    "                    path = os.path.join(root, fname)\n",
    "                    item = (path, class_to_idx[target])\n",
    "                    images.append(item)\n",
    "\n",
    "    return images\n",
    "\n",
    "#制作嵌入背景的数据集\n",
    "class ViewpointDataset(data.Dataset):\n",
    "    def __init__(self, root, background_path,transform = None):\n",
    "        classes, class_to_idx = find_classes(root)\n",
    "        imgs = make_dataset(root,class_to_idx)\n",
    "        if len(imgs) == 0:\n",
    "            raise(RuntimeError(\"Found 0 images in subfolders of: \" + root + \"\\n\"\n",
    "                               \"Supported image extensions are: \" + \",\".join(IMG_EXTENSIONS)))\n",
    "        self.root = root\n",
    "        self.imgs = imgs\n",
    "        self.classes = classes\n",
    "        #todo\n",
    "        self.img_width = 227\n",
    "        self.img_height = 227\n",
    "        self.img_depth = 3\n",
    "        self.img_suffix = '.png'\n",
    "        self.transform = transform\n",
    "        self.background_path = background_path\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.background_path!=None:\n",
    "            img_path, label = self.imgs[idx]\n",
    "            img = imread(img_path)\n",
    "            img = img[:,:,:3]\n",
    "            #r = img[:,:,0]\n",
    "            #img[:,:,0]=img[:,:,2]\n",
    "            #img[:,:,2]=r\n",
    "            #preprocess\n",
    "            img = imresize(img, size=(self.img_height, self.img_width))                      \n",
    "            #add background\n",
    "            bg_img_list= glob.glob(os.path.join(self.background_path, '*.bmp'))\n",
    "            bg_img = imread(random.choice(bg_img_list))\n",
    "            bg_img = imresize(bg_img, size=(300,300))\n",
    "            random_seed_region = np.asarray(np.shape(bg_img)[:2]) - np.asarray([self.img_height, self.img_width])\n",
    "            random_crop_y, random_crop_x = (np.random.random((1,2)) * random_seed_region)[0]\n",
    "\n",
    "            random_crop_y = np.uint16(random_crop_y)\n",
    "            random_crop_x = np.uint16(random_crop_x)\n",
    "            bg_img = bg_img[random_crop_y:random_crop_y+self.img_height, random_crop_x:random_crop_x+self.img_width]\n",
    "            #split the forground and background of the source images by color (0,0,0)\n",
    "            mask = np.float32(np.less_equal(img,2))\n",
    "            img = np.multiply(bg_img , mask) + np.multiply(img, (1-mask)) \n",
    "            #img = np.swapaxes(img,2,0)\n",
    "            #调整亮度\n",
    "            image = img_as_float(img/255.0)\n",
    "            #print 'image',image\n",
    "            r = random.randint(5,20) * 0.1\n",
    "            img = exposure.adjust_gamma(image, r)   #调暗\n",
    "            img = img * 255.0\n",
    "            \n",
    "            img = np.uint8(img)\n",
    "            img = Image.fromarray(img)\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "            #img = np.swapaxes(img,2,0)\n",
    "\n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For datasets data_loader\n",
    "def get_dataloader(case, batch_size,category=None):\n",
    "    print('[INFO] Loading datasets: {}'.format(case))\n",
    "    datas = {\n",
    "        'synthetic': '../dataset/'+category+'/synthetic/',\n",
    "        'real': '../dataset/'+category+'/real3/',\n",
    "        'adaptation': '../dataset/'+category+'/adaptation3/'\n",
    "    }\n",
    "    means = {\n",
    "        'imagenet': [0.485, 0.456, 0.406]\n",
    "    }\n",
    "    stds = {\n",
    "        'imagenet': [0.229, 0.224, 0.225],\n",
    "    }\n",
    "\n",
    "    img_size = (227,227)\n",
    "\n",
    "    transform = [\n",
    "        transforms.Scale(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(means['imagenet'], stds['imagenet']),\n",
    "    ]\n",
    "\n",
    "    data_loader = data.DataLoader(\n",
    "        dataset=datasets.ImageFolder(\n",
    "            datas[case],\n",
    "            transform=transforms.Compose(transform)\n",
    "        ),\n",
    "        num_workers=4,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    return data_loader\n",
    "\n",
    "def get_bg_dataloader(case, batch_size,category=None):\n",
    "    print('[INFO] Loading datasets: {}'.format(case))\n",
    "    datas = {\n",
    "        'synthetic': '../dataset/'+category+'/synthetic/', \n",
    "        'background': '../background/'\n",
    "    }\n",
    "    means = {\n",
    "        'imagenet': [0.485, 0.456, 0.406],\n",
    "    }\n",
    "    stds = {\n",
    "        'imagenet': [0.229, 0.224, 0.225],\n",
    "    }\n",
    "\n",
    "    img_size = (227,227)\n",
    "\n",
    "    transform = [\n",
    "        transforms.Scale(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(means['imagenet'], stds['imagenet']),\n",
    "    ]\n",
    "    data_loader=data.DataLoader(\n",
    "        ViewpointDataset(datas[case],background_path=datas['background'],transform = transforms.Compose(transform)),\n",
    "        num_workers=4,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "def imshow(inp):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1,2,0))\n",
    "    #inp = inp.numpy()\n",
    "    inp = np.uint8(inp)\n",
    "    plt.imshow(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net + Coral loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CUDA = True if torch.cuda.is_available() else False\n",
    "#CUDA =False\n",
    "'''\n",
    "MODELS\n",
    "'''\n",
    "\n",
    "class DoubleStream(nn.Module):\n",
    "    def __init__(self, num_classes=1000,adap_layer = 512):\n",
    "        super(DoubleStream, self).__init__()\n",
    "        #self.sharedNet = AlexNet()\n",
    "        self.sharedNet = VGGnet()\n",
    "        self.source_fc = nn.Linear(4096, adap_layer)\n",
    "        self.target_fc = nn.Linear(4096, adap_layer)\n",
    "        self.classifier = nn.Linear(adap_layer, num_classes)\n",
    "        # initialize according to CORAL paper experiment\n",
    "        self.source_fc.weight.data.normal_(0, 0.005)\n",
    "        self.target_fc.weight.data.normal_(0, 0.005)\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        source = self.sharedNet(source)\n",
    "        source = self.source_fc(source)\n",
    "        classifier = self.classifier(source)\n",
    "        \n",
    "        target = self.sharedNet(target)\n",
    "        target = self.source_fc(target)\n",
    "        target_out  = self.classifier(target)\n",
    "        return source, target, classifier, target_out\n",
    "\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class VGGnet(nn.Module):   \n",
    "    def __init__(self):\n",
    "        super(VGGnet, self).__init__()\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "        self.features = vgg16.features\n",
    "        classifier = vgg16.classifier\n",
    "        bottleneck = list(classifier.children())[:-3]\n",
    "        self.bottleneck = nn.Sequential(*bottleneck)\n",
    "        self.final_fc = nn.Linear(4096,4096)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x1 = self.features(inputs)\n",
    "        x1 = x1.view(x1.size(0), -1) \n",
    "        btn = self.bottleneck(x1)\n",
    "        return  self.final_fc(F.dropout(F.relu(btn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mmd\n",
    "def _mix_rbf_kernel(X, Y, sigma_list):\n",
    "    assert(X.size(0) == Y.size(0))\n",
    "    m = X.size(0)\n",
    "\n",
    "    Z = torch.cat((X, Y), 0)\n",
    "    ZZT = torch.mm(Z, Z.t())\n",
    "    diag_ZZT = torch.diag(ZZT).unsqueeze(1)\n",
    "    Z_norm_sqr = diag_ZZT.expand_as(ZZT)\n",
    "    exponent = Z_norm_sqr - 2 * ZZT + Z_norm_sqr.t()\n",
    "\n",
    "    K = 0.0\n",
    "    for sigma in sigma_list:\n",
    "        gamma = 1.0 / (2 * sigma**2)\n",
    "        K += torch.exp(-gamma * exponent)\n",
    "\n",
    "    return K[:m, :m], K[:m, m:], K[m:, m:], len(sigma_list)\n",
    "\n",
    "\n",
    "def mix_rbf_mmd2(X, Y, sigma_list, biased=True):\n",
    "    K_XX, K_XY, K_YY, d = _mix_rbf_kernel(X, Y, sigma_list)\n",
    "    # return _mmd2(K_XX, K_XY, K_YY, const_diagonal=d, biased=biased)\n",
    "    return _mmd2(K_XX, K_XY, K_YY, const_diagonal=False, biased=biased)\n",
    "\n",
    "def _mmd2(K_XX, K_XY, K_YY, const_diagonal=False, biased=False):\n",
    "    m = K_XX.size(0)    # assume X, Y are same shape\n",
    "\n",
    "    # Get the various sums of kernels that we'll use\n",
    "    # Kts drop the diagonal, but we don't need to compute them explicitly\n",
    "    if const_diagonal is not False:\n",
    "        diag_X = diag_Y = const_diagonal\n",
    "        sum_diag_X = sum_diag_Y = m * const_diagonal\n",
    "    else:\n",
    "        diag_X = torch.diag(K_XX)                       # (m,)\n",
    "        diag_Y = torch.diag(K_YY)                       # (m,)\n",
    "        sum_diag_X = torch.sum(diag_X)\n",
    "        sum_diag_Y = torch.sum(diag_Y)\n",
    "\n",
    "    Kt_XX_sums = K_XX.sum(dim=1) - diag_X             # \\tilde{K}_XX * e = K_XX * e - diag_X\n",
    "    Kt_YY_sums = K_YY.sum(dim=1) - diag_Y             # \\tilde{K}_YY * e = K_YY * e - diag_Y\n",
    "    K_XY_sums_0 = K_XY.sum(dim=0)                     # K_{XY}^T * e\n",
    "\n",
    "    Kt_XX_sum = Kt_XX_sums.sum()                       # e^T * \\tilde{K}_XX * e\n",
    "    Kt_YY_sum = Kt_YY_sums.sum()                       # e^T * \\tilde{K}_YY * e\n",
    "    K_XY_sum = K_XY_sums_0.sum()                       # e^T * K_{XY} * e\n",
    "\n",
    "    if biased:\n",
    "        mmd2 = ((Kt_XX_sum + sum_diag_X) / (m * m)\n",
    "            + (Kt_YY_sum + sum_diag_Y) / (m * m)\n",
    "            - 2.0 * K_XY_sum / (m * m))\n",
    "    else:\n",
    "        mmd2 = (Kt_XX_sum / (m * (m - 1))\n",
    "            + Kt_YY_sum / (m * (m - 1))\n",
    "            - 2.0 * K_XY_sum / (m * m))\n",
    "\n",
    "    return mmd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, epoch, _lambda, _count):\n",
    "    model.train()\n",
    "    \n",
    "    result = []\n",
    "    acc = []\n",
    "    source, target = list(enumerate(source_loader,0)), list(enumerate(target_loader,0))\n",
    "    #train_steps = min(len(source), len(target))\n",
    "    #train_steps = int(min(len(source), len(target)) / 2)\n",
    "    train_steps = len(source)\n",
    "    print len(source)\n",
    "    print len(target)\n",
    "    \n",
    "    for batch_idx in range(train_steps):\n",
    "        model.train()\n",
    "        _, (source_data, source_label) = source[batch_idx]\n",
    "        idx = batch_idx\n",
    "        if batch_idx >= len(target):\n",
    "            idx = batch_idx % len(target)\n",
    "        _, (target_data, _) = target[idx]\n",
    "\n",
    "        if CUDA:\n",
    "            source_data = source_data.cuda()\n",
    "            source_label = source_label.cuda()\n",
    "            target_data = target_data.cuda()\n",
    "\n",
    "        source_data, source_label = Variable(source_data), Variable(source_label)\n",
    "        target_data = Variable(target_data)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out1, out2, classifier, _ = model(source_data, target_data)\n",
    "\n",
    "        classification_loss = torch.nn.functional.cross_entropy(classifier, source_label)\n",
    "        sigma_list=[1,2,4,8,16]\n",
    "        #print(out1.data, out2.data)\n",
    "        if out1.size(0)==out2.size(0):\n",
    "            mmd2_D = mix_rbf_mmd2(out1,out2, sigma_list)\n",
    "            mmd2_D = F.relu(mmd2_D)\n",
    "            mmd_loss=mmd2_D\n",
    "        else:\n",
    "            mmd_loss=Variable(torch.Tensor([0.0]).cuda())\n",
    "        if _lambda == 0:\n",
    "            sum_loss = classification_loss\n",
    "        else:\n",
    "            sum_loss = _lambda*mmd_loss + classification_loss\n",
    "        sum_loss.backward()\n",
    "        if _count <= 1000:\n",
    "            writer.add_scalar('data/Loss',mmd_loss + classification_loss, _count)\n",
    "            writer.add_scalar('data/MMD_loss',mmd_loss, _count)\n",
    "            writer.add_scalar('data/total_loss',sum_loss, _count)\n",
    "        optimizer.step()\n",
    "        result.append({\n",
    "            'epoch': epoch,\n",
    "            'step': batch_idx,\n",
    "            'total_steps': train_steps,\n",
    "            'lambda': _lambda,\n",
    "            'mmd_loss': mmd_loss.data[0],\n",
    "            'classification_loss': classification_loss.data[0],\n",
    "            'total_loss': sum_loss.data[0]\n",
    "        })\n",
    "\n",
    "        if _count% 50 == 0 and _count <= 1000:\n",
    "            test_target = test(model, test_loader, e, mode='target')\n",
    "            print test_target['accuracy']\n",
    "            writer.add_scalar('data/Test_Accuracy',test_target['accuracy'],_count)\n",
    "            \n",
    "        _count += 2\n",
    "\n",
    "    return result,acc,_count\n",
    "\n",
    "\n",
    "\n",
    "def test(model, dataset_loader, e, mode='source'):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in dataset_loader:\n",
    "        if CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        _, _, out1, out2 = model(data, data)\n",
    "\n",
    "        out = out1 if mode == 'source' else out2\n",
    "\n",
    "        # sum up batch loss\n",
    "        test_loss += torch.nn.functional.cross_entropy(out, target, size_average=False).data[0]\n",
    "\n",
    "        # get the index of the max log-probability\n",
    "        #pred = out.data.max(1, keepdim=True)[1]\n",
    "        pred = torch.topk(out.data,2)[1].cpu().numpy()\n",
    "        gt = target.data.cpu().numpy()\n",
    "        for i in range(len(pred)):\n",
    "            if gt[i] == pred[i][0]:\n",
    "                correct += 1\n",
    "\n",
    "    test_loss /= len(dataset_loader.dataset)\n",
    "\n",
    "    return {\n",
    "        'epoch': e,\n",
    "        'average_loss': test_loss,\n",
    "        'correct': correct,\n",
    "        'total': len(dataset_loader.dataset),\n",
    "        'accuracy': 100. * correct / len(dataset_loader.dataset)\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# load AlexNet pre-trained model\n",
    "def load_pretrained(model):\n",
    "    url = 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth'\n",
    "    pretrained_dict = model_zoo.load_url(url)\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    # filter out unmatch dict and delete last fc bias, weight\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "    # del pretrained_dict['classifier.6.bias']\n",
    "    # del pretrained_dict['classifier.6.weight']\n",
    "\n",
    "    model_dict.update(pretrained_dict)\n",
    "    model.load_state_dict(model_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading datasets: synthetic\n",
      "[INFO] Loading datasets: adaptation\n",
      "[INFO] Loading datasets: real\n"
     ]
    }
   ],
   "source": [
    "init_lr = 1e-3\n",
    "WEIGHT_DECAY = 5e-4\n",
    "MOMENTUM = 0.9\n",
    "BATCH_SIZE = [32,32]\n",
    "EPOCHS = 2\n",
    "\n",
    "CATEGORY='wp5'\n",
    "Adaptation = True\n",
    "#Adaptation =False\n",
    "#Background=True\n",
    "Background=False\n",
    "\n",
    "if Background:\n",
    "    source_loader = get_bg_dataloader(case='synthetic', batch_size=BATCH_SIZE[0], category = CATEGORY)\n",
    "else:\n",
    "    source_loader = get_dataloader(case='synthetic', batch_size=BATCH_SIZE[0], category = CATEGORY)\n",
    "target_loader = get_dataloader(case='adaptation', batch_size=BATCH_SIZE[1],category = CATEGORY)\n",
    "test_loader = get_dataloader(case='real', batch_size=BATCH_SIZE[1],category = CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfYAAADWCAYAAAAjMKA+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7EIIIcQC8f8BNXeNFbmTJDkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f226408c990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(source_loader))\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out)\n",
    "plt.title([x for x in classes])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269\n",
      "4\n",
      "11.8120805369\n",
      "65.7718120805\n",
      "69.6644295302\n",
      "75.5704697987\n",
      "73.9597315436\n",
      "73.1543624161\n",
      "73.5570469799\n",
      "73.6912751678\n",
      "73.0201342282\n",
      "73.1543624161\n",
      "72.6174496644\n",
      "269\n",
      "4\n",
      "73.5570469799\n",
      "72.6174496644\n",
      "72.4832214765\n",
      "72.7516778523\n",
      "73.0201342282\n",
      "73.288590604\n",
      "73.288590604\n",
      "73.4228187919\n",
      "73.5570469799\n",
      "73.5570469799\n",
      "ave =  nan\n",
      "lam =  0.0\n",
      "lam_acc =  nan\n",
      "Training complete in 10m 13s\n",
      "269\n",
      "4\n",
      "17.5838926174\n",
      "62.0134228188\n",
      "63.8926174497\n",
      "69.3959731544\n",
      "70.6040268456\n",
      "57.5838926174\n",
      "74.6308724832\n",
      "75.9731543624\n",
      "79.4630872483\n",
      "81.2080536913\n",
      "81.610738255\n",
      "269\n",
      "4\n",
      "82.2818791946\n",
      "83.6241610738\n",
      "83.4899328859\n",
      "84.5637583893\n",
      "84.1610738255\n",
      "84.8322147651\n",
      "85.9060402685\n",
      "85.2348993289\n",
      "84.966442953\n",
      "85.6375838926\n",
      "ave =  nan\n",
      "lam =  1.0\n",
      "lam_acc =  nan\n",
      "Training complete in 24m 6s\n",
      "Training complete in 24m 12s\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    since = time.time()\n",
    "    lam = [0.0,1.0]\n",
    "    na_acc = []\n",
    "    a_acc = []\n",
    "    lam_acc = []\n",
    "    for i in range(2):\n",
    "        if i%1 == 0:\n",
    "            lam_acc = []\n",
    "        model = DoubleStream(7,4096)\n",
    "        writer = SummaryWriter()\n",
    "        if CUDA:\n",
    "            model = model.cuda()\n",
    "\n",
    "        #load_pretrained(model.sharedNet)\n",
    "\n",
    "        _count = 0\n",
    "        final_acc = []\n",
    "        for e in range(0, EPOCHS):\n",
    "            LEARNING_RATE = init_lr * (0.1**(e // 7))\n",
    "            optimizer = torch.optim.SGD([\n",
    "                    {'params': model.sharedNet.parameters()},\n",
    "                    {'params': model.source_fc.parameters(), 'lr': 10*LEARNING_RATE},\n",
    "                    {'params': model.target_fc.parameters(), 'lr': 10*LEARNING_RATE}\n",
    "                ], lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "\n",
    "            if Adaptation:\n",
    "                #_lambda = ((e+1)/3) * (5 / EPOCHS)\n",
    "                ind = int(i/1)\n",
    "                _lambda = lam[ind]\n",
    "            else:\n",
    "                _lambda = 0.0\n",
    "            res, acc, _count = train(model, optimizer, e+1, _lambda, _count)\n",
    "            final_acc = final_acc + acc\n",
    "        print 'ave = ', np.mean(final_acc)\n",
    "        inde = int(i/1)\n",
    "        print 'lam = ', lam[inde]\n",
    "        lam_acc = lam_acc + final_acc\n",
    "        print 'lam_acc = ', np.mean(lam_acc)\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "                time_elapsed // 60, time_elapsed % 60))\n",
    "        model_name='./model/model'+'_' + CATEGORY + '_' +str(i)+'_adap.pth'\n",
    "        torch.save(model,model_name)\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
