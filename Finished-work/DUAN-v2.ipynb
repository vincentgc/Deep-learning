{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms,models\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function, Variable\n",
    "from __future__ import division\n",
    "from torch.utils import model_zoo\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from scipy.misc import imread, imsave, imresize\n",
    "import glob\n",
    "import random\n",
    "from PIL import Image\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorboardX import SummaryWriter\n",
    "from skimage import exposure, img_as_float\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMG_EXTENSIONS = [\n",
    "    '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
    "    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n",
    "]\n",
    "\n",
    "\n",
    "def is_image_file(filename):\n",
    "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
    "\n",
    "def find_classes(dir):\n",
    "    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "    classes.sort()\n",
    "    class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "    return classes, class_to_idx\n",
    "\n",
    "\n",
    "def make_dataset(dir, class_to_idx):\n",
    "    images = []\n",
    "    for target in os.listdir(dir):\n",
    "        d = os.path.join(dir, target)\n",
    "        if not os.path.isdir(d):\n",
    "            continue\n",
    "        for root, _, fnames in sorted(os.walk(d)):\n",
    "            for fname in fnames:\n",
    "                if is_image_file(fname):\n",
    "                    path = os.path.join(root, fname)\n",
    "                    item = (path, class_to_idx[target])\n",
    "                    images.append(item)\n",
    "\n",
    "    return images\n",
    "\n",
    "#制作嵌入背景的数据集\n",
    "class ViewpointDataset(data.Dataset):\n",
    "    def __init__(self, root, background_path,transform = None):\n",
    "        classes, class_to_idx = find_classes(root)\n",
    "        imgs = make_dataset(root,class_to_idx)\n",
    "        if len(imgs) == 0:\n",
    "            raise(RuntimeError(\"Found 0 images in subfolders of: \" + root + \"\\n\"\n",
    "                               \"Supported image extensions are: \" + \",\".join(IMG_EXTENSIONS)))\n",
    "        self.root = root\n",
    "        self.imgs = imgs\n",
    "        self.classes = classes\n",
    "        #todo\n",
    "        self.img_width = 227\n",
    "        self.img_height = 227\n",
    "        self.img_depth = 3\n",
    "        self.img_suffix = '.png'\n",
    "        self.transform = transform\n",
    "        self.background_path = background_path\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.background_path!=None:\n",
    "            img_path, label = self.imgs[idx]\n",
    "            img = imread(img_path)\n",
    "            img = img[:,:,:3]\n",
    "            #r = img[:,:,0]\n",
    "            #img[:,:,0]=img[:,:,2]\n",
    "            #img[:,:,2]=r\n",
    "            #preprocess\n",
    "            img = imresize(img, size=(self.img_height, self.img_width))                      \n",
    "            #add background\n",
    "            bg_img_list= glob.glob(os.path.join(self.background_path, '*.bmp'))\n",
    "            bg_img = imread(random.choice(bg_img_list))\n",
    "            bg_img = imresize(bg_img, size=(300,300))\n",
    "            random_seed_region = np.asarray(np.shape(bg_img)[:2]) - np.asarray([self.img_height, self.img_width])\n",
    "            random_crop_y, random_crop_x = (np.random.random((1,2)) * random_seed_region)[0]\n",
    "\n",
    "            random_crop_y = np.uint16(random_crop_y)\n",
    "            random_crop_x = np.uint16(random_crop_x)\n",
    "            bg_img = bg_img[random_crop_y:random_crop_y+self.img_height, random_crop_x:random_crop_x+self.img_width]\n",
    "            #split the forground and background of the source images by color (0,0,0)\n",
    "            mask = np.float32(np.less_equal(img,2))\n",
    "            img = np.multiply(bg_img , mask) + np.multiply(img, (1-mask)) \n",
    "            #img = np.swapaxes(img,2,0)\n",
    "            #调整亮度\n",
    "            image = img_as_float(img/255.0)\n",
    "            #print 'image',image\n",
    "            r = random.randint(5,20) * 0.1\n",
    "            img = exposure.adjust_gamma(image, r)   #调暗\n",
    "            img = img * 255.0\n",
    "            \n",
    "            img = np.uint8(img)\n",
    "            img = Image.fromarray(img)\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "            #img = np.swapaxes(img,2,0)\n",
    "\n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For datasets data_loader\n",
    "def get_dataloader(case, batch_size,category=None):\n",
    "    print('[INFO] Loading datasets: {}'.format(case))\n",
    "    datas = {\n",
    "        'synthetic': '../../dataset/'+category+'/synthetic/',\n",
    "        'real': '../../dataset/'+category+'/real3/',\n",
    "        'adaptation': '../../dataset/'+category+'/adaptation3/'\n",
    "    }\n",
    "    means = {\n",
    "        'imagenet': [0.485, 0.456, 0.406]\n",
    "    }\n",
    "    stds = {\n",
    "        'imagenet': [0.229, 0.224, 0.225],\n",
    "    }\n",
    "\n",
    "    img_size = (227,227)\n",
    "\n",
    "    transform = [\n",
    "        transforms.Scale(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(means['imagenet'], stds['imagenet']),\n",
    "    ]\n",
    "\n",
    "    data_loader = data.DataLoader(\n",
    "        dataset=datasets.ImageFolder(\n",
    "            datas[case],\n",
    "            transform=transforms.Compose(transform)\n",
    "        ),\n",
    "        num_workers=4,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    return data_loader\n",
    "\n",
    "def get_bg_dataloader(case, batch_size,category=None):\n",
    "    print('[INFO] Loading datasets: {}'.format(case))\n",
    "    datas = {\n",
    "        'synthetic': '../../dataset/'+category+'/synthetic/', \n",
    "        'background': '../../background/'\n",
    "    }\n",
    "    means = {\n",
    "        'imagenet': [0.485, 0.456, 0.406],\n",
    "    }\n",
    "    stds = {\n",
    "        'imagenet': [0.229, 0.224, 0.225],\n",
    "    }\n",
    "\n",
    "    img_size = (227,227)\n",
    "\n",
    "    transform = [\n",
    "        transforms.Scale(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(means['imagenet'], stds['imagenet']),\n",
    "    ]\n",
    "    data_loader=data.DataLoader(\n",
    "        ViewpointDataset(datas[case],background_path=datas['background'],transform = transforms.Compose(transform)),\n",
    "        num_workers=4,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "def imshow(inp):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1,2,0))\n",
    "    #inp = inp.numpy()\n",
    "    inp = np.uint8(inp)\n",
    "    plt.imshow(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net + Coral loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CUDA = True if torch.cuda.is_available() else False\n",
    "#CUDA =False\n",
    "'''\n",
    "MODELS\n",
    "'''\n",
    "\n",
    "class DoubleStream(nn.Module):\n",
    "    def __init__(self, num_classes=1000,adap_layer = 512):\n",
    "        super(DoubleStream, self).__init__()\n",
    "        #self.sharedNet = AlexNet()\n",
    "        self.sharedNet = VGGnet()\n",
    "        self.classifier = nn.Linear(adap_layer, num_classes)\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        source = self.sharedNet(source)\n",
    "        classifier = self.classifier(F.relu(source))\n",
    "        \n",
    "        target = self.sharedNet(target)\n",
    "        target_out  = self.classifier(F.relu(target))\n",
    "        return source, target, classifier, target_out\n",
    "\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class VGGnet(nn.Module):   \n",
    "    def __init__(self):\n",
    "        super(VGGnet, self).__init__()\n",
    "        vgg16 = models.vgg16(pretrained=True)\n",
    "        self.features = vgg16.features\n",
    "        classifier = vgg16.classifier\n",
    "        #print classifier\n",
    "        bottleneck = list(classifier.children())[:-3]\n",
    "        self.bottleneck = nn.Sequential(*bottleneck)\n",
    "        self.final_fc = nn.Linear(4096,512)\n",
    "        self.final_fc.weight.data.normal_(0, 0.005)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x1 = self.features(inputs)\n",
    "        x1 = x1.view(x1.size(0), -1) \n",
    "        btn = self.bottleneck(x1)\n",
    "        return  self.final_fc(F.dropout(F.relu(btn)))\n",
    "        #return btn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mmd\n",
    "def _mix_rbf_kernel(X, Y, sigma_list):\n",
    "    assert(X.size(0) == Y.size(0))\n",
    "    m = X.size(0)\n",
    "\n",
    "    Z = torch.cat((X, Y), 0)\n",
    "    ZZT = torch.mm(Z, Z.t())\n",
    "    diag_ZZT = torch.diag(ZZT).unsqueeze(1)\n",
    "    Z_norm_sqr = diag_ZZT.expand_as(ZZT)\n",
    "    exponent = Z_norm_sqr - 2 * ZZT + Z_norm_sqr.t()\n",
    "\n",
    "    K = 0.0\n",
    "    for sigma in sigma_list:\n",
    "        gamma = 1.0 / (2 * sigma**2)\n",
    "        K += torch.exp(-gamma * exponent)\n",
    "\n",
    "    return K[:m, :m], K[:m, m:], K[m:, m:], len(sigma_list)\n",
    "\n",
    "\n",
    "def mix_rbf_mmd2(X, Y, sigma_list, biased=True):\n",
    "    K_XX, K_XY, K_YY, d = _mix_rbf_kernel(X, Y, sigma_list)\n",
    "    # return _mmd2(K_XX, K_XY, K_YY, const_diagonal=d, biased=biased)\n",
    "    return _mmd2(K_XX, K_XY, K_YY, const_diagonal=False, biased=biased)\n",
    "\n",
    "def _mmd2(K_XX, K_XY, K_YY, const_diagonal=False, biased=False):\n",
    "    m = K_XX.size(0)    # assume X, Y are same shape\n",
    "\n",
    "    # Get the various sums of kernels that we'll use\n",
    "    # Kts drop the diagonal, but we don't need to compute them explicitly\n",
    "    if const_diagonal is not False:\n",
    "        diag_X = diag_Y = const_diagonal\n",
    "        sum_diag_X = sum_diag_Y = m * const_diagonal\n",
    "    else:\n",
    "        diag_X = torch.diag(K_XX)                       # (m,)\n",
    "        diag_Y = torch.diag(K_YY)                       # (m,)\n",
    "        sum_diag_X = torch.sum(diag_X)\n",
    "        sum_diag_Y = torch.sum(diag_Y)\n",
    "\n",
    "    Kt_XX_sums = K_XX.sum(dim=1) - diag_X             # \\tilde{K}_XX * e = K_XX * e - diag_X\n",
    "    Kt_YY_sums = K_YY.sum(dim=1) - diag_Y             # \\tilde{K}_YY * e = K_YY * e - diag_Y\n",
    "    K_XY_sums_0 = K_XY.sum(dim=0)                     # K_{XY}^T * e\n",
    "\n",
    "    Kt_XX_sum = Kt_XX_sums.sum()                       # e^T * \\tilde{K}_XX * e\n",
    "    Kt_YY_sum = Kt_YY_sums.sum()                       # e^T * \\tilde{K}_YY * e\n",
    "    K_XY_sum = K_XY_sums_0.sum()                       # e^T * K_{XY} * e\n",
    "\n",
    "    if biased:\n",
    "        mmd2 = ((Kt_XX_sum + sum_diag_X) / (m * m)\n",
    "            + (Kt_YY_sum + sum_diag_Y) / (m * m)\n",
    "            - 2.0 * K_XY_sum / (m * m))\n",
    "    else:\n",
    "        mmd2 = (Kt_XX_sum / (m * (m - 1))\n",
    "            + Kt_YY_sum / (m * (m - 1))\n",
    "            - 2.0 * K_XY_sum / (m * m))\n",
    "\n",
    "    return mmd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, epoch, _lambda, _count):\n",
    "    model.train()\n",
    "    \n",
    "    result = []\n",
    "    acc = []\n",
    "    source, target = list(enumerate(source_loader,0)), list(enumerate(target_loader,0))\n",
    "    #train_steps = min(len(source), len(target))\n",
    "    #train_steps = int(min(len(source), len(target)) / 2)\n",
    "    train_steps = len(source)\n",
    "    print len(source)\n",
    "    print len(target)\n",
    "    \n",
    "    for batch_idx in range(train_steps):\n",
    "        model.train()\n",
    "        _, (source_data, source_label) = source[batch_idx]\n",
    "        idx = batch_idx\n",
    "        if batch_idx >= len(target):\n",
    "            idx = batch_idx % len(target)\n",
    "        _, (target_data, _) = target[idx]\n",
    "\n",
    "        if CUDA:\n",
    "            source_data = source_data.cuda()\n",
    "            source_label = source_label.cuda()\n",
    "            target_data = target_data.cuda()\n",
    "\n",
    "        source_data, source_label = Variable(source_data), Variable(source_label)\n",
    "        target_data = Variable(target_data)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out1, out2, classifier, _ = model(source_data, target_data)\n",
    "\n",
    "        classification_loss = torch.nn.functional.cross_entropy(classifier, source_label)\n",
    "        if _lambda == 0:\n",
    "            sum_loss = classification_loss\n",
    "            sum_loss.backward()\n",
    "            mmd_loss = Variable(torch.Tensor([0.0]).cuda())\n",
    "        else:\n",
    "            sigma_list=[1,2,4,8,16]\n",
    "            #print(out1.data, out2.data)\n",
    "            if out1.size(0)==out2.size(0):\n",
    "                mmd2_D = mix_rbf_mmd2(out1,out2, sigma_list)\n",
    "                mmd2_D = F.relu(mmd2_D)\n",
    "                mmd_loss=mmd2_D\n",
    "            else:\n",
    "                mmd_loss=Variable(torch.Tensor([0.0]).cuda())\n",
    "            #mmd = _lambda*mmd_loss\n",
    "            sum_loss = _lambda*mmd_loss + classification_loss\n",
    "            sum_loss.backward()\n",
    "            #mmd.backward()\n",
    "        '''if _count % 200 == 0:\n",
    "            monitor = [0,14,24]\n",
    "            fc = [3]\n",
    "            for moni in monitor:\n",
    "                print \"{} - conv {}`s grad = \".format(_count, moni), model.sharedNet.features[moni].weight.grad[0][:3]\n",
    "                print \"{} - conv {}`s data = \".format(_count, moni), model.sharedNet.features[moni].weight.data[0][:3]\n",
    "            for f  in fc:\n",
    "                print \"{} - fc {}`s grad = \".format(_count, f), model.sharedNet.bottleneck[f].weight.grad[:3]\n",
    "                print \"{} - fc {}`s data = \".format(_count, f), model.sharedNet.bottleneck[f].weight.data[:3]\n",
    "            print \"source_fc`s grad = \", model.source_fc.weight.grad[:3]\n",
    "            print \"source_fc`s data = \", model.source_fc.weight.data[:3]\n",
    "        '''\n",
    "        if _count <= 1000:\n",
    "            if _lambda != 0:\n",
    "                writer.add_scalar('data/MMD_loss',mmd_loss, _count)\n",
    "            writer.add_scalar('data/total_loss',sum_loss, _count)\n",
    "        optimizer.step()\n",
    "        result.append({\n",
    "            'epoch': epoch,\n",
    "            'step': batch_idx,\n",
    "            'total_steps': train_steps,\n",
    "            'lambda': _lambda,\n",
    "            'mmd_loss': mmd_loss.data[0],\n",
    "            'classification_loss': classification_loss.data[0],\n",
    "            'total_loss': sum_loss.data[0]\n",
    "        })\n",
    "\n",
    "        if _count% 200 == 0 and _count <= 1000:\n",
    "            test_target = test(model, test_loader, e, mode='target')\n",
    "            print test_target['accuracy']\n",
    "            writer.add_scalar('data/Test_Accuracy',test_target['accuracy'],_count)\n",
    "            \n",
    "        _count += 2\n",
    "\n",
    "    return result,acc,_count\n",
    "\n",
    "\n",
    "def test(model, dataset_loader, e, mode='source'):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in dataset_loader:\n",
    "        if CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        _, _, out1, out2 = model(data, data)\n",
    "\n",
    "        out = out1 if mode == 'source' else out2\n",
    "\n",
    "        # sum up batch loss\n",
    "        test_loss += torch.nn.functional.cross_entropy(out, target, size_average=False).data[0]\n",
    "\n",
    "        # get the index of the max log-probability\n",
    "        #pred = out.data.max(1, keepdim=True)[1]\n",
    "        pred = torch.topk(out.data,2)[1].cpu().numpy()\n",
    "        gt = target.data.cpu().numpy()\n",
    "        for i in range(len(pred)):\n",
    "            if gt[i] == pred[i][0]:\n",
    "                correct += 1\n",
    "\n",
    "    test_loss /= len(dataset_loader.dataset)\n",
    "\n",
    "    return {\n",
    "        'epoch': e,\n",
    "        'average_loss': test_loss,\n",
    "        'correct': correct,\n",
    "        'total': len(dataset_loader.dataset),\n",
    "        'accuracy': 100. * correct / len(dataset_loader.dataset)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading datasets: synthetic\n",
      "[INFO] Loading datasets: adaptation\n",
      "[INFO] Loading datasets: real\n"
     ]
    }
   ],
   "source": [
    "init_lr = 1e-3\n",
    "WEIGHT_DECAY = 5e-4\n",
    "MOMENTUM = 0.9\n",
    "BATCH_SIZE = [32,32]\n",
    "EPOCHS = 2\n",
    "\n",
    "CATEGORY='wp1'\n",
    "Adaptation = True\n",
    "#Adaptation =False\n",
    "#Background=True\n",
    "Background=False\n",
    "\n",
    "if Background:\n",
    "    source_loader = get_bg_dataloader(case='synthetic', batch_size=BATCH_SIZE[0], category = CATEGORY)\n",
    "else:\n",
    "    source_loader = get_dataloader(case='synthetic', batch_size=BATCH_SIZE[0], category = CATEGORY)\n",
    "target_loader = get_dataloader(case='adaptation', batch_size=BATCH_SIZE[1],category = CATEGORY)\n",
    "test_loader = get_dataloader(case='real', batch_size=BATCH_SIZE[1],category = CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfYAAADWCAYAAAAjMKA+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnXncLFdZ579P1UsSScjKNcabwA0SiMEPkHAHM7IMGAxJ\nRg37J8JA0DgZZ0DR6EAUdeLHFXVkZFQ0QzBBwzYI5jLjKItEEU0kN4SEbOQSA0nMcgnZzEJ8u575\n45zqrq5e3q53qTrd/fveT923uqpO11NPnXN+Z3mq2twdIYQQQiwGWdcGCCGEEGLzkLALIYQQC4SE\nXQghhFggJOxCCCHEAiFhF0IIIRYICbsQQgixQKwp7GbmZvaQmf1qGwYJIYQQYhgze4mZ/YuZFWb2\nkmnHztpjf5a7vz1++dPM7BIz22tm3zCzvzKzp89o2L5mdoGZfdXMHjSzq8zs1BltqH7PPmZ2vZnd\n1iDNd0Vbv25mjR/eN7PHm9kfxPT3m9nfNkj7y2Z2jZmtmtl5Dc97oZk9Fm9oueQzpn2xmX0m2ntL\nk/PG9K+N9+ohM/tzMzt0Hd/xi7FxODUj1tI828x2m9nD8e+zG6TdiK/3NbP3mtkDZnanmZ3TJH38\njlbzppkdZ2ZXmNm9cfmUmR3XIP2OmEceNrMbGt6nS83s0Uq+vLFB2t82s5tiPXCDmb2hQdqlK8sx\n/Qlm9rcx3V1m9pYZ07042nyfmd1jZh8zs+0zpn2imX0uprvPzP7BzJ7XwOZD4/keinXJaxuk3Ujd\n91NmdnMsy/9sZu80s5UZ0x5hZrtiOjezHQ1sXrc+xvQT/eXun3L3A4CvrfU96xmKPxjYBTwdOBz4\nR+CSGdOuALcC/w44CPh54MNNHBf5r8Dehmn+FfgwcFbDdCXnA4cC3xn//lSDtHuAtwL/d53n/k13\nP6Cy9GZM9xDwXoK/GmFmzwD+CHg94T4/DPxBw+/4DuDVwB0N0uxDyE9/ChwCXARcErfPwkZ8fR5w\nDPBk4MXAW83slIbf0Xbe/GfgVYQ8+URC2fxgg/QfAL4AHAa8HfiImW1rkP7NlXw5cwVGyJs/QKgH\nzgR+18y+Z8a0S1eWzeyJwF8SyuRhwFOBT8x4zuuAl7r7wcC3AzcB754x7b8APwJsI5THdwAfn1Uk\ngd8HHiPUIa8D3h3rlllZb923CzjB3Q8Evgt4FvATM6YtCL5+ZQM7Szaij7BxfwXcfeoCOPDUKfsP\njcccttZ3TUh/NfDKBscfDVwPnArcto7zPTVcdqM0xwIPAAeu5xor3/OnwHkN01wI/MoGz/sS4JaG\naX4NeH/l83fEDPeEBt/xl8BpwC3AS2ZMczJwO2CVbV8DTmnB1/8MnFz5/MvABxukbz1v1tKvAG8C\nHp7x+KcB36zeU+CzwI/NmP5S4EfXa2/tu3YBP73V/prXshzL459sgp/3BX4duG4daTNCY8yBb53h\n+P1jnfG0yrY/AX5jq/1V+57DgE8Bf9Aw3Uq81h0bOPfM+jirv2apTzcjeO6FwJ3ufk/ThGZ2OKFy\nubZBsv8J/BzwSNPzbYDnAl8FfikO311jZutpza2X/xKHdXa3eN5nAF8sP7j7V4iZbpbEZvZq4Jvu\n/hfrOO/VHnNw5Oq4fcsws0OAI6hcc1xvct4u8iYAZnYf8Gi04ddmTPYM4GZ3f7Cyrek1/3osE58z\nsxc1SNfHzL4F+Dc0qwfWy7yW5ROBb5jZ35vZ3Wb2cTN70qyJzexJMY88AvwM8JtNjDazqwn5axfw\nHne/e4ZkTwNW3f3LlW1N89e66z4LU4kPAF8n9Nj/qEn6TaKJPm6Gv4ANRsWb2ZGEoYP1zEU+DrgY\nuMjdb5gxzcuB3N0/1vR8G+RIwnDO/YShrDcDF5nZd7Zw7ncRhoe/FfgF4MImc1wb4ADC9Va5H3jC\nWgnN7AkEcZlpDnCzzrtBDqicq/F5O8ybAHgYZj2IkDe/MGOyjfr6bcBTgO2E4e2Px+mXpvwhoQL7\nq3Wkbcq8luUjCVMWbwGeBPwTYRplJtz9azGPPJEwBTpTnVtJ/0zgQOC1wN/NmOwAwuhIlSb5a0N1\nn7u/38NQ/NMIeeyuWdNuBuvQx436q8+6hT3Ow32CMLwxcwaLaTPCEMNjhII1S5r9Ca3MWedJNpNH\nCPN6v+Luj7n73wCfIQwbbynufqW73+Puq7H3ezHwiq0+L2Fu7cDatgOBB8ccW+c8wrDhLS2fdyP8\nS+Vcjc7bcd7s4+4PESqw95nZt86QZEO+dvfL3f1Bd/+mu18EfI4w9TIzZvZbBKF9TW2UZquY17L8\nCPAxd/+8uz8K/BLwPWZ2UEMbvsEgbmXWefIy7aOxrj/XzJ41Q5KN5q9Nqfvc/SbCaFCjGKGNsE59\n3LS6b13CHoctPwHscvdGj8GZmQEXEIIDXunu/zpj0mOAHcBnzexO4KPAERail3c0sWEdXD1mW1c/\ni+eAtXCeawnDVwCY2VMI83NfnphiwEnAT8R7cydwFCFI8m0znveZMZ+UPJMtHqZ193sJQX7VCutZ\nM563y7xZJwMeT+hFr8W1wFPiCEvJrNc8jkZ508x+iRCPcLK713sqW8W8luWrGbZzIzavEHrBdRGZ\nlccRRmrW4svAipkdU9nWWv6qsUKIE9pyNqCPm+evGSb0h4LnCJnhH4Hfm3D8i5gS0ELoUVwGHDDl\nfC+aEMjwbZXlFYRgp28jDIFCCCp444TvNWA/4Lh4jv2AfSv7LwQunJD2cYRo2F+IdjyP0Io6Nu5/\nI1OC02L6/YD3A78S10ubdzAlQIMQ8XwAocI+OZ73RWv5K+7L4rlOJcwr7gfsU9l/KRMCgAjzOg8A\nLyAEdfwplUCyNfx1WO1e3UqIjj8g7j8PuHRC2n2irW8hNCTeHD/v04KvfwP4G0L077EEoT+lsj/F\nvPl9wPFATiib74rn3m9Gf10G/HY858uB+4Bta5VlQvTvS2O6FUIE70PEwJ8ZfP2zhOjsb5uwf6v8\nNa9l+XuBe4FnRxveCXx2xrL8CkKUdkaIbv8wcGVl/3lMLo8nAs8nlMtvIUy/PAh8+1p5JO7/IGHK\nYP/o6/uBZ7Tgrx8lBvjFfHIt8Duz+Cvu3y/a7NF3+83or43q40R/1crG1OC5iTtqzqsK+5lx20OE\noYNyeVLc/3rgcxO+68kx7aO1tK+L+48iiMksEYQvohJ5HDNev4COOb7MRNXllsr+TwP/ccr5ngH8\nQ7zu64CXV/b9AnDxlLQXjjn3G+O+F8Qb9bgJaT8bb+4DhHnIMyr7pvqrzES15dLK/q8A3zfF7tcS\nItIfIjyyceis/pqWEQkjNr865fjjgd2E4ccrgeNb8vW+hMcDHyDMx50zq6+7ypuEBtMNhHK0l/AY\n1jMb+GsHoZJ7BLixdp+mleVtwOfjdd1HaCB8X2X/Wr52QkR+tR74OZXlyfkL+M+EJ0buBT4OHDVL\nWQZ+nDAn/xBwJ0E8njxLeSQ8mvzFeD++QWj4vnCWPBL3Hwr8eTz314DXNsgjG6n7/phQhh+K5/gt\nhsV5rbqvfo99Rn+tWx/X8lflmFvYBGF/NDr3l9c6Nh7/HsIzk2seOybtfwB+fZ1pnw98YJ1p9yE8\npjQ2g82Q/hPAd64z7c8D/6kDfx0J/H1H/rpqWgW2gL6e17y5kbK8EV/Pq7/mrizH9Bspj13lkWX0\n10mERvQjwIunHWsxgRBCCCEWAP0IzBqY2SlmdqOZ7TGzc7u2RwghhJiGeuxTiO8l/jIhQOk2wpzi\nD7n7dZ0aJoQQQkxAPfbpPBfY4+43u/tjhKCT0zu2SQghhJhIoxcULCHbCY9qldwGfHf9IDM7Gzgb\nYP/993/Oscce2451Igl2797Nc57znK7NWBPZKVJm9+7dX3f3Jj9AJCagofgpmNmrCM8x/2j8/Hrg\nu9194tvydu7c6Z///BVtmSgSIMuMoki/HMlOkTJZZrvdfWfXdiwCGoqfzu2E5yVLjozbhBBCiCSR\nsE/n88AxZnZ0/D3wMwi/biSEEEIkiebYp+Duq2b2ZsKvTuXAe929jZ+WFEIIIdaFhH0NPPyqUNPf\nFBdCCCE6QUPxQgghxAIhYRdCCCEWCAm7EEIIsUBI2IUQQogFQsIuhBBCLBCKihdLgxnoRYubS55n\nYGG9t1p0a4wQApCwzwcGeWaA0SsKSFmcoq29XjpGrqwMBqZWExefLDPMDHB6had9r6Ev6ikSfAlF\n4WMbdF039Cz6bl4am2axIQcU7hQJlXExjIQ9MfI8w92H3pVdinp/f+HJvku7tDXPM3q9tEU0Jcwg\ny7Io6gCWXANpnsjzgS+zzEYaSUMNqA58nOeDMl3i7nFp3Zw1qTaOgUo+FSmiOfaEKIc1LTPyWJCG\nKoD4J6vs75I8t2gfWFaxNf5JwcZ5IM+NLBtfUU7aLiZT9ioHWGxwDvZXG1Bt+zirNNSrjV8zI8sy\n8nywdH3/s3x8XWOM87NIBfXYk8KptuJDgRoW9Woxz1eyzuY1QyNk0CMKqzYyNNuljSXDXk2LsmE0\nHsPMK/4NqBc/mYE/6361ib3ktke/Rnu7k3OomVUEtN3RhTw3bFrJserIR0CjdGmgJldCFIWPTriN\ndtiHpl27azUPrLCyBzKhDuiy555lluQ8tdm43njd0FDh13Wg615cFass1Z5mubQ2Ymv1RtK4m969\nqNfvXSi/YUpgWkYNP69trZX3IV9OCPUwJl2P6BrdhYTwqOs+pbLxMR+6qOh7vVppn2aCr9Uz3Tzy\nPCNfGSz13m4qolj2cibPp453rjvJxFeYDeTI4+f60hbZ1JONimYXog7j56a9b1saeROq+XLUR/16\nKn4eXJJTFOqxp4CEPTHKysbH9N7HiXqX3dGZht365o32PLcCs+Fe5Lj9KVAVldG77COfS0FPRdQh\nZs/qMoY2gqzKxtpoI2m8LwsvOvHjuEalu8fh7ul+GjQE27O7Xt84NR/H9fDUQZgmSDHwbxmRsCdI\nWXh9pCSNHAl024ObOn9eqxlS6C2nFM3bv29Tbl/ZO5r0yFbytODuoUbSNF8SHtPyjjqVs+Q9d6dX\nFLEOGL2YoqVMUAw9RTApn3pf1FNqcAoJe5IMDbeajR+aL18KkkAg1VhxH6oUwodeC4W/e280Z1x9\n3x/unKNKs96BbzNocS1xL6e4uhL1ib11G/5cFdRwHbWLaTErDB69Gz99MNcNzgVHwp4wReF44Vg2\nXtyT6n3Wh+UNqrXQyJz8FjEtPiE16vP/JWVFqUqzGaW4131aNpC6fRlNMKooitoU1sDYcQ24Lm2u\nR7wH4tRQYtNCYhgJe+L0xd3KefewvYyjbSsobS3GB/0ZTrujCtU5yEnTvym0h6aJ+jz10ocY02Vv\nW5jGiXpKDaTwrPr6MmCb8+uDaP3h87ujufQ5QM+xzwFF4ZUAoSjumfVrza5fjVlS2mn9R8za6aVX\nGRLy6rkrdZSZdSqc1VeJVtfLijOFe7k+uvVpXTBTeoKgKIramwXLXvxgwqK038pYOh8elSt868tT\nNvbZdU/Kl2Jt1GOfE8oglZJq7ziFoLSSwbP4g0ZHq/aN7ab7sMZ33GWvnn/ug+MSoR5nmpoQlQ23\ncdtLzAajOMaYkaU2Lqc+pR+j3VPypVgbCfscUa+sUi1q9aH31sV9ZEwYcF/jFSDtUb2HqT3CthEG\nAX/DwX/tnHzQOEpN1EvG2TTayLTa0i5F4ZT/er1uHgsUG0dD8XNIEefcLT60nWJvr9fzofn/NjvJ\nXg3HrvaIkpD1wCJWmCkEf6Xu115RkGdp96f0q23zj4R9Tqn2SlIT9ZK6uLfFpMo9pSkLsaT4YL69\npNcrJryK1WMchrUaOCfmHwm72FJ6vRBQl3pPSqwPCU5zBs+nh4bmuB+nCQzm3FV+RBMk7GLLSalS\nGnqrn9gwKd3beSKMZpW99OkjSWo8iaZI2MXSkGIsglhehofkfSjYUPlUbAQJuxBCdEB42Usxsk2I\njZJ2eKYQQgghGiFhF0IIIRaIpRJ2MzvKzD5jZteZ2bVm9pa4/VAz+6SZ3RT/HhK3m5m9y8z2mNnV\nZnZCt1cghBBCTGephB1YBX7a3Y8DTgTeZGbHAecCn3b3Y4BPx88ApwLHxOVs4N3tmyyEEELMzlIJ\nu7vf4e5XxvUHgeuB7cDpwEXxsIuAl8X104H3eeAy4GAzO6Jls4UQQoiZWSphr2JmO4DjgcuBw939\njrjrTuDwuL4duLWS7La4rf5dZ5vZFWZ2xd69e7fMZiGEEGItllLYzewA4M+An3T3B6r73Cs/TTYj\n7n6+u+90953btm3bREuFEEKIZiydsJvZ4wiifrG7fzRuvqscYo9/747bbweOqiQ/Mm4TQgghkmSp\nhN3CbyReAFzv7r9T2bULODOunwlcUtn+hhgdfyJwf2XIXgghhEiOZXvz3POA1wPXmNlVcdvPAb8B\nfNjMzgK+Crwm7vsL4DRgD/Aw8MPtmiuEEEI0Y6mE3d3/jsm/uHDSmOMdeNOWGiWEEEJsIks1FC+E\nEEIsOhJ2IYQQYoGQsAshhBALhIRdCCGEWCAk7EIIIcQCIWEXQgghFggJuxBCCLFASNiFEEKIBULC\nLoQQQiwQEnYhhBBigZCwCyGEEAvEUr0rfp6w2hvtvf+fWGTy3MCM3mrRtSkTMQMzAyt/eMEAxx2K\nIo1MWtpoBr1eGjZNI88z3D0Z/4n5RsKeGFlmFVGvqHu5Gst9r5dQxR/q9SSZ6M8hvPPK3yxU7haN\nzfNs5B5nWdjXVeVf9WX4W/WnYebkuXXuS4AsG/gyaQxW8iz404wss5H7u7KSsdpRQy9fyQZVj8fG\nm3uy5V0EJOxzihl4x4VrWDTno2eUIllmfdEuKYW+FPc8D8d0fc+nk3ALL0Hq990AKp+r+/OVrPVR\nnCw3Mgt5rjoCspIPZnDLkUSNNqSF5tjnlBQqeO8bYcCoOIm1yfMMy2ysHJbinq9k/bkZM+TnhqTo\nrzw3bIxdRhDxcY29tilHPHq9YmJ9EwcayDJjZSVjJdouukU9dtGIoR7GPJTfhG3M89Agwof+DFPZ\nEHpH6hk1JrE80L/vTLjnMCT6Dp3EXJQW5CvZmmXdY689hQaJkLCLGSlb5fX5anevzbuKWchyG67a\nazX8OOkupvScumG8MQPhCvuLwrfU7uFRjLQzYfANhPHrmAOMoRCa+hV0HUg5i0dHYy5El0jYE2Ck\nFzxNIWN9mefD861bHdASzrVWwe22YNfn/KFu0cT+Ud+frfWGvVqhD8TdJ7iwC1GvD/uvt+GWZVsb\nUBfmficbVxaprhtFWVZ9kmCYcq66fhldBc1NI0zBjSlr0vVk0Bx7csxQOmJNZdlgyTPdytnaFV5b\nKsmt2qPaWoqi3hCboDoeemxdi1Jz0jI4hQj58p4PPOODP2NucIqivrpa0Os5vV6hKaGEUY9dbIgE\n6su5JUxjVHo+9XcXFOnOpzdpaJSPSXWHgaXhx6II89DDozUljnvIDyne93E2TRwDS2CEZJmRsCfA\nLIV4dNira0VNr9QWMwz3DuIBai8GKD+1KEJe6bDV72dRON5x5R4CopqmGfXfVvszBG1NH7HqurRU\nKRt0440K71ToXBQ36DCz0efxRXtI2OeEekHvptDMf0Et/TjJf13MY9dbbb3VIhlPN81nXQhS5yLY\nEJ/UzYU0RB3IbDS6vRrDUq5P0v/O+x1LjoRdbAJTaioxkfL1sVW6joCeX8a+CWDq3q7I+09EDEjp\ndbzAxOJcvvp2XPBcaX8IElR90CUSdjEz5RuoYLiirBZhy8ClTTPhxKlfS3s+fRFIRWbGBWem+MbG\ncmqoWubr++pk1RctpTDssMQolFrMTDl/6mMDy8OH0b6ImESICXCJ+iZQ1ZHR5x0CXXci66Lunqao\nQ2gIFYU3/k0KI+3rWhYk7GJmxjXC+2I/Yb+YTq8nUd8MypfgVBudXsuQXQecVsWuNweNufI1sU1Y\nT2NAbD4aiheNqFdG/bqzjPDu/NEmsayME8q23kswK73y/QWJl5HVXjH0Yy91er0iBtAN/Jvic/fL\nylL22M0sN7MvmNn/iZ+PNrPLzWyPmX3IzPaJ2/eNn/fE/Tu6tHsekKiLlCh78dMi0VtlXsqHB6Fe\nnfDGw3qAnEQ9LZZS2IG3ANdXPr8DeKe7PxW4Fzgrbj8LuDduf2c8TggxJ3h98l00w0PvfHV1+E1z\nZoNoGol6eiydsJvZkcC/B94TPxvwvcBH4iEXAS+L66fHz8T9J1nXE3VCiJnRCNLmURQeevFR5EvB\nF+mxdMIO/A/grUCZIw8D7nP31fj5NmB7XN8O3AoQ998fjx/CzM42syvM7Iq9e/dupe1JUsRAoNSD\ngcRyovy5+Wz1L/aJjbFUwm5m3w/c7e67N/N73f18d9/p7ju3bdu2mV8thBBCNGLZouKfB/ygmZ0G\n7AccCPwucLCZrcRe+ZHA7fH424GjgNvMbAU4CLinfbOFEEKI2ViqHru7/6y7H+nuO4AzgL9299cB\nnwFeFQ87E7gkru+Kn4n7/9rrD8cKIYQQCbFUwj6FtwHnmNkewhz6BXH7BcBhcfs5wLkd2SeEEELM\nxLINxfdx90uBS+P6zcBzxxzzKPDqVg0TQgghNoB67EIIIcQCIWEXQgghFggJuxBCCLFASNiFEEKI\nBULCLoQQQiwQEnYhhBBigZCwCyGEEAuEhF0IIYRYICTsQgghxAIhYRdCCCEWCAm7EEIIsUBI2IUQ\nQogFQsIuhBBCLBASdiGEEGKBkLALIYQQC4SEXQghhFggVro2QMw3WWaYQVE47l1bI9omy4wsMwBW\nV4uOrZlfssyw6Eccej35Uqwf9djFujELC0CWZeS5dWvQjJhBnluS9uYrGXmeQXqmjWA1G1Pz57R7\nXLe9K8zCPe+LOoBV8kGClH5N7X6LAeqxJ0bZ+8Em1+3uULhDxz3kLAsVT+GQmQNGnme4O0XRffc9\nz21kJMGs9LHFY7IkekehsRErcoM8G7UrNKQsCd9CsKWaSS0VtWS4kZFlwz6zzMgMer1u/TjUSwc8\n2miVOiBfyfAijfIk5oc0m4RiKmaQdVyJhooziHlWM8XMOu9tlBV7ltlIJT9y7Er3tmZ5Fr0ZqQp9\n5RgbY38X2EB7kiPc70HjrRyhKddLF3bZ4xzppRMEfVyxtszIV7KxeVeIcajHLtZJKezlepWwvcve\ne1H4UEU4XInbyGpXPfeBCAVKcfdoW57b8JwHYTWVeAaPBqciOYP7XHpykE/rQt6FD/v3cxJT9lmM\nZ0ip995v4FXsTsm+ZUU99kRZq6LssvCEnmS1f+kMzwsMi1CeW+u9DfdJQ62jol6a3vbc9pDQ+PDq\noOc+bFDRK5IR9T4Vg1ZWsqEla7FXPJrHqg3PUTvaLkNTRd19aktjZJg+EcrgybLtWR0dEd0hYReN\nCJXnUL+yj7v3l8H+cgi0m2CbUXGPn2uiXpK3VHEOi5CP2DKuiu+tJijqTO/5tjVllE0Yxh54ctjI\nTubXo4FFr+gLNTBw4NSefDw0gd5winlQDCNh75hyDrgfZRo7F2uVnepjRm0S6h7Dh/qVMUitcpwP\nlX7rf+5e3G3Q5hhxctjQll+HK8hRca/S06NkkylFb2KhqTdAt9SaNTEbNEJ8FlFnMNTtXRsv5gIJ\ne2IMh/1MOa6DYa+h+ct+/RIU0j30JuqCPswsV7Y1BHG3cQMNEe//38YQbeGD8w3bMPq0Q8qiPkuH\nfKs77dVRgbrujdPBzqaxSg3PKkPyzpCD3D2MzBQ+tncuXRezIGEXMzFrL3ZsL5Sy7qoLf7v0esWE\nmrEi6m0N0XoUmLFPLcbtsZJPmrIn2aEJdaEunwR1H25UOF0/4rZGq8NH818KQ+91vNO7LWZBwt4x\n5XPW1SVF1tfrqnePrfOhxFCx120o5z677ckNbXLvzp4G9LNF5b56bWmDkV64j+bZrv251tm7Lhsz\nM4uZip/rFD3ulgDrHRps67GnsoIsezvDjxTR39breb9ADyrV4WC7FOquXs9HHjPrsiYaEaA5fD1v\nvXfcRaeu+ohj3addv4wGqD3uwGgBrmfHCXGfXVM1uX5JJRNnvEQrLF2P3cwONrOPmNkNZna9mf1b\nMzvUzD5pZjfFv4fEY83M3mVme8zsajM7oWv7q7RV+YfAnfDSmcGLPqx2/hgAmGX9ALvhoj2pCuiG\nXm90WqDtl+rk+Wgkd7CrVTPWRWlj2SitXsdgcH4Qf9EmSYr6DJRPjuS5hWgUqz8fns60TH90sTLt\nkfqo4zKxdMIO/C7wl+5+LPAs4HrgXODT7n4M8On4GeBU4Ji4nA28u31zuydU3qOldbjDMU64q+Le\n/TB8nWIk2K89cR8X9DgvAjSJtoff64x75C11n468hneohTT85p/Eis+AsVNJ7ZshBiyVsJvZQcAL\ngQsA3P0xd78POB24KB52EfCyuH468D4PXAYcbGZHtGx2EgzmpqvD8FbruVer9dHeWmrCDkHch3tC\n7dg47Jf0BWgtvNJt8ziVUBTe2nWNC+5MzafV/N9bjYGc48asa28anCdCNkjL78vIUgk7cDSwF/hj\nM/uCmb3HzPYHDnf3O+IxdwKHx/XtwK2V9LfFbUOY2dlmdoWZXbF3794tNL9bBhXl8BvnhuugcQ/s\nlc/gbrGB68QdekUBtCdE5Tx6b45/4KMeHV0Owxbxmtq83/3HB+M5UxN1GM7/4S2HZUBKN/ZsFuX9\nLopBo050y7IJ+wpwAvBudz8eeIjBsDsA7uXM0ey4+/nuvtPdd27btm3TjE2R8D71suducVtV8KsD\nshaHGrscoJ2RDnrN5eNu84oXo0PJXT4jXp47RVEvCT11ZhfzUiWllqIByybstwG3ufvl8fNHCEJ/\nVznEHv/eHfffDhxVSX9k3LbU1CvOQd1eFfyBoPuYNGIxqA67FgmIzzyMfvR6Bb3VytIr4m8AjGno\nWb3xLMTaLJWwu/udwK1m9vS46STgOmAXcGbcdiZwSVzfBbwhRsefCNxfGbJfagY99+rvmw+G6Iui\noNcLQ9tdPz8sto7ytwHmffShU8opjJ6Pin5chGjCMj7H/uPAxWa2D3Az8MOEBs6Hzews4KvAa+Kx\nfwGcBuxtvWSkAAANeElEQVQBHo7HisjgefBBBa+Rw+VC93k5cffRiH6RDEsn7O5+FbBzzK6Txhzr\nwJu23Kg5pjpEqEpeiOXAfT6mPZaVpRqKF0IIIRYdCbsQQgixQEjYhRBCiAVCwi6EEEIsEBJ2IYQQ\nYoGQsAshhBALhIRdCCGEWCAk7EIIIcQCIWEXQgghFggJuxBCCLFASNiFEEKIBULCLoQQQiwQEnYh\nhBBigZCwCyGEEAuEhF0IIYRYICTsQgghxAIhYRdCCCEWCAm7EEIIsUBI2IUQQogFQsIuhBBCLBAS\ndiESxaxrC4QQ88hK1waIyeR5qNndoSi8Y2smk2WGmVEUBZ6umXNDngd/lhSFJ33/5wEzlDfF0iBh\nT5QsN8qq3SxU9ikKfCnqYT0MACUl8AaZDQtl37EO7umIZpbZkD+dYGqWGXme0esVndoHo6MINmZY\nIRV/QjV/Or1eanYx3iYL+4uE7DWL99oGxaekKDyd8i4ACXuSxPJDUThm1q9MS4EH6BUeav4uiYXd\nY6muC3yvKDq1saw8pw1pjxOmtsnzDIuVeUloxBV9YTKDlZWM1V53Pg15b9hf4/yXQiOktHVgn5Hn\nloS4l/kSgp1VYTQDy0KjPiV7mVKOsizNTscyozn2BCkr+GpFXyfPrC/yXZFHAZ8kjnmWkefKYpPI\ncmNlJRtbYQahz0Z8u5JnU/PF1hIacT6he1bd12XeHNcAKUmgHTcigFZpgJaiXtJ1GRfziWrdxKgX\n5LUqoq4Kfl2wp/V8807FaG26qOzz3MimnNiZ3kNaWWm36M7qo8Fx7Ts1G2rsjmt8WH+6I8+zTkWz\n2jYygqBnNVEvkbiLpkjYE6KJ+HU57NWkF14fpm+fSu+t3pGLn7M8I1/pZnRhdbUYupd9f82Qtk1x\nL/Om1eMVKoTtg31t+nN82Rkv7tXh+a4anCO99jWO71zc16pu1PZIiqUTdjP7KTO71sy+ZGYfMLP9\nzOxoM7vczPaY2YfMbJ947L7x8564f8dW2VWdd5v1+C6Yh6H1LDOyvBLlM3lktjPKHlv1vrv7TA2g\novAOptnX58C2BampXyZNK7RB08Z516Ne7pOfLCiDPLu2UQTSr6U3ETPbDvwEsNPdvwvIgTOAdwDv\ndPenAvcCZ8UkZwH3xu3vjMdtkW3jt08rTGul3WymFdpJc69t9tRHBD1hSl+VwXHhs40cEwKrar07\nGz9k2z6lbdMyaDuW9kVyxJRJtjm9XrdPb8xStodYIxB0MyiDOKtLk7RlgG/nIwxLzlIJe2QF+BYz\nWwEeD9wBfC/wkbj/IuBlcf30+Jm4/yTbAqWaVAh6Pe8/w9z14yTVx7DqhAoyGNhVD8jG9crj42xe\nTF8oj2nR9vqp3Icr7dXVgl7PY4NpuHfX9qzGuMq9tCusT7/vbY3yDPJguWV8puj1iiSizWH2XrvH\n/7a6obyp3y9t74ylEnZ3vx34beBrBEG/H9gN3Ofuq/Gw24DtcX07cGtMuxqPP6z+vWZ2tpldYWZX\n7N27t5FNTVq2E4fBtrwZP/kco5VpN/TP75VlDOMef+vK9lnO27VfYfq9HzQ8Rw0tioLyRrTRGKk+\nMjZOVQpPR9CrrGWT9/+DIoEMUY40rLWI7lgqYTezQwi98KOBbwf2B07Z6Pe6+/nuvtPdd27btm0d\n6TcWDLfVlWYZvV3tmSfJjEI56RLavraJj43Nmn7zTFkXVfMnu87W2N8Gcdi9+/f7TGSSuA+Jegvv\nriin1MZNrdX3FZWG3bil8wy6xCzbC2peAvyTu+8FMLOPAs8DDjazldgrPxK4PR5/O3AUcFscuj8I\nuGezjSrf6lQX6PoQaFeB5e5OFp9Jr/feSrtT0PuNPinQ9jUUhY+NbK96uAyuG2taCwZPynP17eOe\nHS9fVNRGg6n+ch+zELOQ1FsQ16DX86E3TlZp6ymYuq+Gn9hoxQSxCSxVj50wBH+imT0+zpWfBFwH\nfAZ4VTzmTOCSuL4rfibu/2vf5FpqePhwmP5LKyYEzbRV0NaaQw2iX74hbdjQXjHo5afwcpBUGSeU\nwadx/5jqvo3bPzl4avAseJhDn3xzt1KU6m/sK3Gn8+C49VB/jeyEWQUhprJUwu7ulxOC4K4EriFc\n//nA24BzzGwPYQ79gpjkAuCwuP0c4NzNtqko1v8O6zaFMtg5bSxzvDH5lKA7MSD8iI4PfR5eBsdO\njgDfEssoh7In759MF1M38/6jOSFocvDZSOA5djFXLNtQPO7+34D/Vtt8M/DcMcc+Cry6Dbt6PV93\n4W0zIKjXKxpGOQ8PkYphqtHwWWb9H36Z5fhqZPrWYuuObN9qga1e/zyLeZ2iGK0PUpnyEumzVD32\n1Ckfb5uVMNzYfklvGkTnvlZvf3npP1VA/NGfNY4v4zHaEvXBvV77ZCFKfnCf27rn895Dn8RIz10j\nX2JGlq7HnjqlWFdb6/VfeYNuBL1KUfjIs+3jevMS9OmUAXRFr1iz4u5KwOrnnPQjK+2NICwPZc9d\nv54mmiBhT5Rez/sR0fXXzXYt6iVF4WCDX3kLEb3DQi9mo/5DOaEHn+YjWkP5zwa/d19GwUP3P9m7\nSKRS3sX8IGFPmNBTH476Ta6Q+6CnPhD19QcELiOrqwX5SgY+/JbBufCgj38rnnruQnSHhD1xuppH\nb0qvV6hS3wC91dA1n3ffzbv9QiwCEnaxaahSF0KI7lFUvBBCCLFASNiFEEKIBULCLoQQQiwQEnYh\nhBBigZCwCyGEEAuEhF0IIYRYICTsQgghxAIhYRdCCCEWCAm7EEIIsUBI2IUQQogFwpr8rrZYGzN7\nELixazvWwROBr3dtREPm0WaQ3W0yjzbDfNq9UZuf7O7bNsuYZUbvit98bnT3nV0b0RQzu2Le7J5H\nm0F2t8k82gzzafc82ryoaCheCCGEWCAk7EIIIcQCIWHffM7v2oB1Mo92z6PNILvbZB5thvm0ex5t\nXkgUPCeEEEIsEOqxCyGEEAuEhF0IIYRYICTsm4iZnWJmN5rZHjM7t2t7SszsKDP7jJldZ2bXmtlb\n4vbzzOx2M7sqLqdV0vxsvI4bzeylHdp+i5ldE+27Im471Mw+aWY3xb+HxO1mZu+Kdl9tZid0YO/T\nK/68ysweMLOfTNHXZvZeM7vbzL5U2dbYt2Z2Zjz+JjM7syO7f8vMboi2fczMDo7bd5jZIxW//2El\nzXNi3toTr81atrlxnmi7jplg94cqNt9iZlfF7Un4WgDurmUTFiAHvgI8BdgH+CJwXNd2RduOAE6I\n608AvgwcB5wH/MyY44+L9u8LHB2vK+/I9luAJ9a2/SZwblw/F3hHXD8N+H+AAScClyeQJ+4Enpyi\nr4EXAicAX1qvb4FDgZvj30Pi+iEd2H0ysBLX31Gxe0f1uNr3/GO8FovXdmrLNjfKE13UMePsru3/\n78AvpuRrLa4e+ybyXGCPu9/s7o8BHwRO79gmANz9Dne/Mq4/CFwPbJ+S5HTgg+7+TXf/J2AP4fpS\n4XTgorh+EfCyyvb3eeAy4GAzO6ILAyMnAV9x969OOaYzX7v73wLfGGNPE9++FPiku3/D3e8FPgmc\n0rbd7v4Jd1+NHy8Djpz2HdH2A939Mg/K8z4G17rpTPD1JCblidbrmGl2x173a4APTPuOtn0tNBS/\nmWwHbq18vo3p4tkJZrYDOB64PG56cxy+fG857Epa1+LAJ8xst5mdHbcd7u53xPU7gcPjekp2A5zB\ncKWXuq+huW9Tsx/gRwi9wpKjzewLZvY3ZvaCuG07wdaSruxukidS8/ULgLvc/abKtpR9vTRI2JcI\nMzsA+DPgJ939AeDdwHcAzwbuIAyrpcbz3f0E4FTgTWb2wurO2ANI7plNM9sH+EHgf8dN8+DrIVL1\n7TTM7O3AKnBx3HQH8CR3Px44B3i/mR3YlX015i5P1PghhhuuKft6qZCwbx63A0dVPh8ZtyWBmT2O\nIOoXu/tHAdz9LnfvuXsB/C8GQ8DJXIu73x7/3g18jGDjXeUQe/x7dzw8GbsJDZEr3f0umA9fR5r6\nNhn7zeyNwPcDr4uNEuJw9j1xfTdhjvpp0cbqcH3rdq8jT6Tk6xXgFcCHym0p+3rZkLBvHp8HjjGz\no2Nv7QxgV8c2Af25sAuA6939dyrbq/PPLwfKyNddwBlmtq+ZHQ0cQwh+aRUz29/MnlCuEwKkvhTt\nK6OvzwQuieu7gDfECO4Tgfsrw8ptM9SbSd3XFZr69q+Ak83skDiUfHLc1ipmdgrwVuAH3f3hyvZt\nZpbH9acQ/HtztP0BMzsxlo83MLjWtmxumidSqmNeAtzg7v0h9pR9vXR0Hb23SAshcvjLhJbq27u2\np2LX8wlDqlcDV8XlNOBPgGvi9l3AEZU0b4/XcSMdRbASon+/GJdrS58ChwGfBm4CPgUcGrcb8PvR\n7muAnR3ZvT9wD3BQZVtyviY0PO4A/pUw73nWenxLmNPeE5cf7sjuPYT55zJ//2E89pUx71wFXAn8\nQOV7dhLE9CvA7xHfxNmizY3zRNt1zDi74/YLgR+rHZuEr7W4XikrhBBCLBIaihdCCCEWCAm7EEII\nsUBI2IUQQogFQsIuhBBCLBASdiGEEGKBkLALIYQQC4SEXQghhFgg/j817m7Fg3rNKAAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd3e6ca4ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(source_loader))\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out)\n",
    "plt.title([x for x in classes])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269\n",
      "4\n",
      "15.3635116598\n",
      "68.1755829904\n",
      "68.0384087791\n",
      "269\n",
      "4\n",
      "66.5294924554\n",
      "65.7064471879\n",
      "65.7064471879\n",
      "ave =  nan\n",
      "lam =  0.0\n",
      "lam_acc =  nan\n",
      "Training complete in 7m 57s\n",
      "269\n",
      "4\n",
      "15.9122085048\n",
      "66.6666666667\n",
      "67.9012345679\n",
      "269\n",
      "4\n",
      "68.3127572016\n",
      "68.1755829904\n",
      "67.9012345679\n",
      "ave =  nan\n",
      "lam =  0.0\n",
      "lam_acc =  nan\n",
      "Training complete in 15m 53s\n",
      "269\n",
      "4\n",
      "15.6378600823\n",
      "80.2469135802\n",
      "86.5569272977\n",
      "269\n",
      "4\n",
      "86.4197530864\n",
      "87.3799725652\n",
      "85.5967078189\n",
      "ave =  nan\n",
      "lam =  1.0\n",
      "lam_acc =  nan\n",
      "Training complete in 27m 32s\n",
      "269\n",
      "4\n",
      "16.049382716\n",
      "75.0342935528\n",
      "76.6803840878\n",
      "269\n",
      "4\n",
      "76.817558299\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-500708e08daa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0m_lambda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mfinal_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_acc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m'ave = '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-1a3c551e3dad>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, epoch, _lambda, _count)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_count\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_lambda\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/MMD_loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmmd_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/total_loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msum_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorboardX/writer.pyc\u001b[0m in \u001b[0;36madd_scalar\u001b[0;34m(self, tag, scalar_value, global_step)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0mglobal_step\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGlobal\u001b[0m \u001b[0mstep\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mto\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \"\"\"\n\u001b[0;32m--> 272\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalar_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__append_to_scalar_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalar_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorboardX/summary.pyc\u001b[0m in \u001b[0;36mscalar\u001b[0;34m(name, scalar, collections)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m     85\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_clean_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mscalar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmakenp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scalar should be 0D'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mscalar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorboardX/x2num.pyc\u001b[0m in \u001b[0;36mmakenp\u001b[0;34m(x, modality)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'torch'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpytorch_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodality\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'chainer'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mchainer_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodality\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorboardX/x2num.pyc\u001b[0m in \u001b[0;36mpytorch_np\u001b[0;34m(x, modality)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'IMG'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/tensor.pyc\u001b[0m in \u001b[0;36mcpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;34mr\"\"\"Returns a CPU copy of this tensor if it's not already on the CPU\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/cuda/__init__.pyc\u001b[0m in \u001b[0;36mtype\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0m__new__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lazy_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/_utils.pyc\u001b[0m in \u001b[0;36m_type\u001b[0;34m(self, new_type, async)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot cast dense tensor to sparse tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    since = time.time()\n",
    "    lam = [0.0,1.0]\n",
    "    na_acc = []\n",
    "    a_acc = []\n",
    "    lam_acc = []\n",
    "    for i in range(4):\n",
    "        #model_name='./model/model'+'_' + CATEGORY + '_' +str(i) + '.pkl'\n",
    "        if i%2 == 0:\n",
    "            lam_acc = []\n",
    "        model = DoubleStream(7,512)\n",
    "        #torch.save(model.state_dict(),model_name)#保存模型参数\n",
    "        model.load_state_dict(torch.load('../../model/model_wp1_0.pkl'))#载入模型参数\n",
    "        writer = SummaryWriter()\n",
    "        if CUDA:\n",
    "            model = model.cuda()\n",
    "\n",
    "        LEARNING_RATE = init_lr\n",
    "        optimizer = torch.optim.SGD([\n",
    "                {'params': model.sharedNet.parameters()},\n",
    "                #{'params': model.source_fc.parameters(), 'lr': LEARNING_RATE}\n",
    "                {'params':model.classifier.parameters()}\n",
    "            ], lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay = 1e-4)\n",
    "\n",
    "        '''\n",
    "        optimizer = torch.optim.Adam([\n",
    "                {'params': model.sharedNet.parameters()},\n",
    "                {'params': model.source_fc.parameters(), 'lr': 10*LEARNING_RATE},\n",
    "                {'params': model.target_fc.parameters(), 'lr': 10*LEARNING_RATE}\n",
    "            ], lr=LEARNING_RATE)\n",
    "        '''\n",
    "        _count = 0\n",
    "        final_acc = []\n",
    "        for e in range(0, EPOCHS):\n",
    "            if Adaptation:\n",
    "                ind = int(i/2)\n",
    "                _lambda = lam[ind]\n",
    "            else:\n",
    "                _lambda = 0.0\n",
    "            res, acc, _count = train(model, optimizer, e+1, _lambda, _count)\n",
    "            final_acc = final_acc + acc\n",
    "        print 'ave = ', np.mean(final_acc)\n",
    "        inde = int(i/2)\n",
    "        print 'lam = ', lam[inde]\n",
    "        lam_acc = lam_acc + final_acc\n",
    "        print 'lam_acc = ', np.mean(lam_acc)\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "                time_elapsed // 60, time_elapsed % 60))\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
