{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目录\n",
    "* VGG\n",
    "* Googlenet\n",
    "* Resnet\n",
    "* Densenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGNet\n",
    "vggNet 是第一个真正意义上的深层网络结构，其是 ImageNet2014年的冠军，得益于 python 的函数和循环，我们能够非常方便地构建重复结构的深层网络。\n",
    "\n",
    "vgg 的网络结构非常简单，就是不断地堆叠卷积层和池化层，下面是一个简单的图示\n",
    "\n",
    "![](https://ws4.sinaimg.cn/large/006tNc79ly1fmpk5smtidj307n0dx3yv.jpg)\n",
    "\n",
    "vgg 几乎全部使用 3 x 3 的卷积核以及 2 x 2 的池化层，使用小的卷积核进行多层的堆叠和一个大的卷积核的感受野是相同的，同时小的卷积核还能减少参数，同时可以有更深的结构。\n",
    "\n",
    "vgg 的一个关键就是使用很多层 3 x 3 的卷积然后再使用一个最大池化层，这个模块被使用了很多次"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以定义一个 vgg 的 block，传入三个参数，第一个是模型层数，第二个是输入的通道数，第三个是输出的通道数，第一层卷积接受的输入通道就是图片输入的通道数，然后输出最后的输出通道数，后面的卷积接受的通道数就是最后的输出通道数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "def vgg_block(num_convs, in_channels, out_channels):\n",
    "    net = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), nn.ReLU(True)] # 定义第一层\n",
    "    \n",
    "    for i in range(num_convs-1): # 定义后面的很多层\n",
    "        net.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "        net.append(nn.ReLU(True))\n",
    "        \n",
    "    net.append(nn.MaxPool2d(2, 2)) # 定义池化层\n",
    "    return nn.Sequential(*net)\n",
    "def vgg_stack(num_convs, channels):\n",
    "    net = []\n",
    "    for n, c in zip(num_convs, channels):\n",
    "        in_c = c[0]\n",
    "        out_c = c[1]\n",
    "        net.append(vgg_block(n, in_c, out_c))\n",
    "    return nn.Sequential(*net)\n",
    "\n",
    "vgg_net = vgg_stack((2, 2, 2, 3, 3), ((3, 64), (64, 128), (128, 256), (256, 512), (512, 512)))\n",
    "#vgg_net = torchvision.models.vgg16()\n",
    "#print(vgg_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg(\n",
      "  (feature): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace)\n",
      "      (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace)\n",
      "      (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace)\n",
      "      (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace)\n",
      "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): ReLU(inplace)\n",
      "      (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace)\n",
      "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): ReLU(inplace)\n",
      "      (6): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (3): ReLU(inplace)\n",
      "    (4): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#输入227*227->4096\n",
    "class vgg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(vgg, self).__init__()\n",
    "        self.feature = vgg_net\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(4096, 1000)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "print vgg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GoogLeNet\n",
    "GoogLeNet，这是 Google 的研究人员提出的网络结构，在当时取得了非常大的影响，因为网络的结构变得前所未有，它颠覆了大家对卷积网络的串联的印象和固定做法，采用了一种非常有效的 inception 模块，得到了比 VGG 更深的网络结构，但是却比 VGG 的参数更少，因为其去掉了后面的全连接层，所以参数大大减少，同时有了很高的计算效率。\n",
    "\n",
    "![](https://ws2.sinaimg.cn/large/006tNc79ly1fmprhdocouj30qb08vac3.jpg)\n",
    "\n",
    "这是 googlenet 的网络示意图，下面我们介绍一下其作为创新的 inception 模块。\n",
    "## Inception 模块\n",
    "在上面的网络中，我们看到了多个四个并行卷积的层，这些四个卷积并行的层就是 inception 模块，可视化如下\n",
    "\n",
    "![](https://ws4.sinaimg.cn/large/006tNc79gy1fmprivb2hxj30dn09dwef.jpg)\n",
    "一个 inception 模块的四个并行线路如下：\n",
    "\n",
    "1.一个 1 x 1 的卷积，一个小的感受野进行卷积提取特征\n",
    "\n",
    "2.一个 1 x 1 的卷积加上一个 3 x 3 的卷积，1 x 1 的卷积降低输入的特征通道，减少参数计算量，然后接一个 3 x 3 的卷积做一个较大感受野的卷积\n",
    "\n",
    "3.一个 1 x 1 的卷积加上一个 5 x 5 的卷积，作用和第二个一样\n",
    "\n",
    "4.一个 3 x 3 的最大池化加上 1 x 1 的卷积，最大池化改变输入的特征排列，1 x 1 的卷积进行特征提取\n",
    "\n",
    "最后将四个并行线路得到的特征在通道这个维度上拼接在一起，下面我们可以实现一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: 3 x 96 x 96\n",
      "output shape: 256 x 96 x 96\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "# 定义一个卷积加一个 relu 激活函数和一个 batchnorm 作为一个基本的层结构\n",
    "def conv_relu(in_channel, out_channel, kernel, stride=1, padding=0):\n",
    "    layer = nn.Sequential(\n",
    "        nn.Conv2d(in_channel, out_channel, kernel, stride, padding),\n",
    "        nn.BatchNorm2d(out_channel, eps=1e-3),\n",
    "        nn.ReLU(True)\n",
    "    )\n",
    "    return layer\n",
    "\n",
    "class inception(nn.Module):\n",
    "    def __init__(self, in_channel, out1_1, out2_1, out2_3, out3_1, out3_5, out4_1):\n",
    "        super(inception, self).__init__()\n",
    "        # 第一条线路\n",
    "        self.branch1x1 = conv_relu(in_channel, out1_1, 1)\n",
    "        \n",
    "        # 第二条线路\n",
    "        self.branch3x3 = nn.Sequential( \n",
    "            conv_relu(in_channel, out2_1, 1),\n",
    "            conv_relu(out2_1, out2_3, 3, padding=1)\n",
    "        )\n",
    "        \n",
    "        # 第三条线路\n",
    "        self.branch5x5 = nn.Sequential(\n",
    "            conv_relu(in_channel, out3_1, 1),\n",
    "            conv_relu(out3_1, out3_5, 5, padding=2)\n",
    "        )\n",
    "        \n",
    "        # 第四条线路\n",
    "        self.branch_pool = nn.Sequential(\n",
    "            nn.MaxPool2d(3, stride=1, padding=1),\n",
    "            conv_relu(in_channel, out4_1, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        f1 = self.branch1x1(x)\n",
    "        f2 = self.branch3x3(x)\n",
    "        f3 = self.branch5x5(x)\n",
    "        f4 = self.branch_pool(x)\n",
    "        output = torch.cat((f1, f2, f3, f4), dim=1)\n",
    "        return output\n",
    "\n",
    "test_net = inception(3, 64, 48, 64, 64, 96, 32)\n",
    "test_x = Variable(torch.zeros(1, 3, 96, 96))\n",
    "print('input shape: {} x {} x {}'.format(test_x.shape[1], test_x.shape[2], test_x.shape[3]))\n",
    "test_y = test_net(test_x)\n",
    "print('output shape: {} x {} x {}'.format(test_y.shape[1], test_y.shape[2], test_y.shape[3]))\n",
    "#可以看到输入经过了 inception 模块之后，大小没有变化，通道的维度变多了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "googlenet(\n",
      "  (block1): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "      (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True)\n",
      "      (2): ReLU(inplace)\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
      "  )\n",
      "  (block2): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True)\n",
      "      (2): ReLU(inplace)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      (2): ReLU(inplace)\n",
      "    )\n",
      "    (2): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
      "  )\n",
      "  (block3): Sequential(\n",
      "    (0): inception(\n",
      "      (branch1x1): Sequential(\n",
      "        (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True)\n",
      "        (2): ReLU(inplace)\n",
      "      )\n",
      "      (branch3x3): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (branch5x5): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (branch_pool): Sequential(\n",
      "        (0): MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), ceil_mode=False)\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): inception(\n",
      "      (branch1x1): Sequential(\n",
      "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True)\n",
      "        (2): ReLU(inplace)\n",
      "      )\n",
      "      (branch3x3): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (branch5x5): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(32, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (branch_pool): Sequential(\n",
      "        (0): MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), ceil_mode=False)\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
      "  )\n",
      "  (block4): Sequential(\n",
      "    (0): inception(\n",
      "      (branch1x1): Sequential(\n",
      "        (0): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "        (2): ReLU(inplace)\n",
      "      )\n",
      "      (branch3x3): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(208, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (branch5x5): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(16, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (branch_pool): Sequential(\n",
      "        (0): MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), ceil_mode=False)\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): inception(\n",
      "      (branch1x1): Sequential(\n",
      "        (0): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True)\n",
      "        (2): ReLU(inplace)\n",
      "      )\n",
      "      (branch3x3): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(224, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (branch5x5): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(24, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (branch_pool): Sequential(\n",
      "        (0): MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), ceil_mode=False)\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): inception(\n",
      "      (branch1x1): Sequential(\n",
      "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True)\n",
      "        (2): ReLU(inplace)\n",
      "      )\n",
      "      (branch3x3): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (branch5x5): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(24, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (branch_pool): Sequential(\n",
      "        (0): MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), ceil_mode=False)\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): inception(\n",
      "      (branch1x1): Sequential(\n",
      "        (0): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True)\n",
      "        (2): ReLU(inplace)\n",
      "      )\n",
      "      (branch3x3): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (branch5x5): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (branch_pool): Sequential(\n",
      "        (0): MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), ceil_mode=False)\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): inception(\n",
      "      (branch1x1): Sequential(\n",
      "        (0): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True)\n",
      "        (2): ReLU(inplace)\n",
      "      )\n",
      "      (branch3x3): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (branch5x5): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (branch_pool): Sequential(\n",
      "        (0): MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), ceil_mode=False)\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
      "  )\n",
      "  (block5): Sequential(\n",
      "    (0): inception(\n",
      "      (branch1x1): Sequential(\n",
      "        (0): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True)\n",
      "        (2): ReLU(inplace)\n",
      "      )\n",
      "      (branch3x3): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (branch5x5): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (branch_pool): Sequential(\n",
      "        (0): MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), ceil_mode=False)\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): inception(\n",
      "      (branch1x1): Sequential(\n",
      "        (0): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True)\n",
      "        (2): ReLU(inplace)\n",
      "      )\n",
      "      (branch3x3): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(832, 182, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(182, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(182, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (branch5x5): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(48, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "          (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "      (branch_pool): Sequential(\n",
      "        (0): MaxPool2d(kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), ceil_mode=False)\n",
      "        (1): Sequential(\n",
      "          (0): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True)\n",
      "          (2): ReLU(inplace)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): AvgPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=1024, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class googlenet(nn.Module):\n",
    "    def __init__(self, in_channel, num_classes, verbose=False):\n",
    "        super(googlenet, self).__init__()\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.block1 = nn.Sequential(\n",
    "            conv_relu(in_channel, out_channel=64, kernel=7, stride=2, padding=3),\n",
    "            nn.MaxPool2d(3, 2)\n",
    "        )\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            conv_relu(64, 64, kernel=1),\n",
    "            conv_relu(64, 192, kernel=3, padding=1),\n",
    "            nn.MaxPool2d(3, 2)\n",
    "        )\n",
    "        \n",
    "        self.block3 = nn.Sequential(\n",
    "            inception(192, 64, 96, 128, 16, 32, 32),\n",
    "            inception(256, 128, 128, 192, 32, 96, 64),\n",
    "            nn.MaxPool2d(3, 2)\n",
    "        )\n",
    "        \n",
    "        self.block4 = nn.Sequential(\n",
    "            inception(480, 192, 96, 208, 16, 48, 64),\n",
    "            inception(512, 160, 112, 224, 24, 64, 64),\n",
    "            inception(512, 128, 128, 256, 24, 64, 64),\n",
    "            inception(512, 112, 144, 288, 32, 64, 64),\n",
    "            inception(528, 256, 160, 320, 32, 128, 128),\n",
    "            nn.MaxPool2d(3, 2)\n",
    "        )\n",
    "        \n",
    "        self.block5 = nn.Sequential(\n",
    "            inception(832, 256, 160, 320, 32, 128, 128),\n",
    "            inception(832, 384, 182, 384, 48, 128, 128),\n",
    "            nn.AvgPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(1024, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        if self.verbose:\n",
    "            print('block 1 output: {}'.format(x.shape))\n",
    "        x = self.block2(x)\n",
    "        if self.verbose:\n",
    "            print('block 2 output: {}'.format(x.shape))\n",
    "        x = self.block3(x)\n",
    "        if self.verbose:\n",
    "            print('block 3 output: {}'.format(x.shape))\n",
    "        x = self.block4(x)\n",
    "        if self.verbose:\n",
    "            print('block 4 output: {}'.format(x.shape))\n",
    "        x = self.block5(x)\n",
    "        if self.verbose:\n",
    "            print('block 5 output: {}'.format(x.shape))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "print googlenet(3,10,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block 1 output: torch.Size([1, 64, 23, 23])\n",
      "block 2 output: torch.Size([1, 192, 11, 11])\n",
      "block 3 output: torch.Size([1, 480, 5, 5])\n",
      "block 4 output: torch.Size([1, 832, 2, 2])\n",
      "block 5 output: torch.Size([1, 1024, 1, 1])\n",
      "output: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "test_net = googlenet(3, 10, True)\n",
    "test_x = Variable(torch.zeros(1, 3, 96, 96))\n",
    "test_y = test_net(test_x)\n",
    "print('output: {}'.format(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "\n",
    "ResNet 有效地解决了深度神经网络难以训练的问题，可以训练高达 1000 层的卷积网络。网络之所以难以训练，是因为存在着梯度消失的问题，离 loss 函数越远的层，在反向传播的时候，梯度越小，就越难以更新，随着层数的增加，这个现象越严重。之前有两种常见的方案来解决这个问题：\n",
    "\n",
    "1.按层训练，先训练比较浅的层，然后在不断增加层数，但是这种方法效果不是特别好，而且比较麻烦\n",
    "\n",
    "2.使用更宽的层，或者增加输出通道，而不加深网络的层数，这种结构往往得到的效果又不好\n",
    "\n",
    "ResNet 通过引入了跨层链接解决了梯度回传消失的问题。 \n",
    "\n",
    "\n",
    "![](https://ws1.sinaimg.cn/large/006tNc79ly1fmptq2snv9j30j808t74a.jpg)\n",
    "\n",
    "\n",
    "这就普通的网络连接跟跨层残差连接的对比图，使用普通的连接，上层的梯度必须要一层一层传回来，而是用残差连接，相当于中间有了一条更短的路，梯度能够从这条更短的路传回来，避免了梯度过小的情况。\n",
    "\n",
    "假设某层的输入是 x，期望输出是 H(x)， 如果我们直接把输入 x 传到输出作为初始结果，这就是一个更浅层的网络，更容易训练，而这个网络没有学会的部分，我们可以使用更深的网络 F(x) 去训练它，使得训练更加容易，最后希望拟合的结果就是 F(x) = H(x) - x，这就是一个残差的结构\n",
    "\n",
    "残差网络的结构就是上面这种残差块的堆叠，下面让我们来实现一个 residual block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([1, 32, 96, 96])\n",
      "output: torch.Size([1, 32, 96, 96])\n",
      "input: torch.Size([1, 3, 96, 96])\n",
      "output: torch.Size([1, 32, 48, 48])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def conv3x3(in_channel, out_channel, stride=1):\n",
    "    return nn.Conv2d(in_channel, out_channel, 3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "class residual_block(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, same_shape=True):\n",
    "        super(residual_block, self).__init__()\n",
    "        self.same_shape = same_shape\n",
    "        stride=1 if self.same_shape else 2\n",
    "        \n",
    "        self.conv1 = conv3x3(in_channel, out_channel, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        \n",
    "        self.conv2 = conv3x3(out_channel, out_channel)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "        if not self.same_shape:\n",
    "            self.conv3 = nn.Conv2d(in_channel, out_channel, 1, stride=stride)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = F.relu(self.bn1(out), True)\n",
    "        out = self.conv2(out)\n",
    "        out = F.relu(self.bn2(out), True)\n",
    "        \n",
    "        if not self.same_shape:\n",
    "            x = self.conv3(x)\n",
    "        return F.relu(x+out, True)\n",
    "#test\n",
    "# 输入输出形状相同\n",
    "test_net = residual_block(32, 32)\n",
    "test_x = Variable(torch.zeros(1, 32, 96, 96))\n",
    "print('input: {}'.format(test_x.shape))\n",
    "test_y = test_net(test_x)\n",
    "print('output: {}'.format(test_y.shape))\n",
    "# 输入输出形状不同\n",
    "test_net = residual_block(3, 32, False)\n",
    "test_x = Variable(torch.zeros(1, 3, 96, 96))\n",
    "print('input: {}'.format(test_x.shape))\n",
    "test_y = test_net(test_x)\n",
    "print('output: {}'.format(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet(\n",
      "  (block1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2))\n",
      "  (block2): Sequential(\n",
      "    (0): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
      "    (1): residual_block(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "    (2): residual_block(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "  )\n",
      "  (block3): Sequential(\n",
      "    (0): residual_block(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv3): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "    )\n",
      "    (1): residual_block(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "  )\n",
      "  (block4): Sequential(\n",
      "    (0): residual_block(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
      "    )\n",
      "    (1): residual_block(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "  )\n",
      "  (block5): Sequential(\n",
      "    (0): residual_block(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
      "    )\n",
      "    (1): residual_block(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "    (2): AvgPool2d(kernel_size=3, stride=3, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class resnet(nn.Module):\n",
    "    def __init__(self, in_channel, num_classes, verbose=False):\n",
    "        super(resnet, self).__init__()\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.block1 = nn.Conv2d(in_channel, 64, 7, 2)\n",
    "        \n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.MaxPool2d(3, 2),\n",
    "            residual_block(64, 64),\n",
    "            residual_block(64, 64)\n",
    "        )\n",
    "        \n",
    "        self.block3 = nn.Sequential(\n",
    "            residual_block(64, 128, False),\n",
    "            residual_block(128, 128)\n",
    "        )\n",
    "        \n",
    "        self.block4 = nn.Sequential(\n",
    "            residual_block(128, 256, False),\n",
    "            residual_block(256, 256)\n",
    "        )\n",
    "        \n",
    "        self.block5 = nn.Sequential(\n",
    "            residual_block(256, 512, False),\n",
    "            residual_block(512, 512),\n",
    "            nn.AvgPool2d(3)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        if self.verbose:\n",
    "            print('block 1 output: {}'.format(x.shape))\n",
    "        x = self.block2(x)\n",
    "        if self.verbose:\n",
    "            print('block 2 output: {}'.format(x.shape))\n",
    "        x = self.block3(x)\n",
    "        if self.verbose:\n",
    "            print('block 3 output: {}'.format(x.shape))\n",
    "        x = self.block4(x)\n",
    "        if self.verbose:\n",
    "            print('block 4 output: {}'.format(x.shape))\n",
    "        x = self.block5(x)\n",
    "        if self.verbose:\n",
    "            print('block 5 output: {}'.format(x.shape))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "print resnet(3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block 1 output: torch.Size([1, 64, 45, 45])\n",
      "block 2 output: torch.Size([1, 64, 22, 22])\n",
      "block 3 output: torch.Size([1, 128, 11, 11])\n",
      "block 4 output: torch.Size([1, 256, 6, 6])\n",
      "block 5 output: torch.Size([1, 512, 1, 1])\n",
      "output: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "#test    \n",
    "test_net = resnet(3, 10, True)\n",
    "test_x = Variable(torch.zeros(1, 3, 96, 96))\n",
    "test_y = test_net(test_x)\n",
    "print('output: {}'.format(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet\n",
    "因为 ResNet 提出了跨层链接的思想，这直接影响了随后出现的卷积网络架构，其中最有名的就是 cvpr 2017 的 best paper，DenseNet。\n",
    "\n",
    "DenseNet 和 ResNet 不同在于 ResNet 是跨层求和，而 DenseNet 是跨层将特征在通道维度进行拼接，下面可以看看他们两者的图示\n",
    "\n",
    "![](https://ws4.sinaimg.cn/large/006tNc79ly1fmpvj5vkfhj30uw0anq73.jpg)\n",
    "\n",
    "![](https://ws1.sinaimg.cn/large/006tNc79ly1fmpvj7fxd1j30vb0eyzqf.jpg)\n",
    "\n",
    "第一张图是 ResNet，第二张图是 DenseNet，因为是在通道维度进行特征的拼接，所以底层的输出会保留进入所有后面的层，这能够更好的保证梯度的传播，同时能够使用低维的特征和高维的特征进行联合训练，能够得到更好的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: 3 x 96 x 96\n",
      "output shape: 39 x 96 x 96\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "def conv_block(in_channel, out_channel):\n",
    "    layer = nn.Sequential(\n",
    "        nn.BatchNorm2d(in_channel),\n",
    "        nn.ReLU(True),\n",
    "        nn.Conv2d(in_channel, out_channel, 3, padding=1, bias=False)\n",
    "    )\n",
    "    return layer\n",
    "#dense block 将每次的卷积的输出称为 `growth_rate`，因为如果输入是 `in_channel`，有 n 层，那么输出就是 `in_channel + n * growh_rate`\n",
    "class dense_block(nn.Module):\n",
    "    def __init__(self, in_channel, growth_rate, num_layers):\n",
    "        super(dense_block, self).__init__()\n",
    "        block = []\n",
    "        channel = in_channel\n",
    "        for i in range(num_layers):\n",
    "            block.append(conv_block(channel, growth_rate))\n",
    "            channel += growth_rate\n",
    "            \n",
    "        self.net = nn.Sequential(*block)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.net:\n",
    "            out = layer(x)\n",
    "            x = torch.cat((out, x), dim=1)\n",
    "        return x\n",
    "    \n",
    "test_net = dense_block(3, 12, 3)\n",
    "test_x = Variable(torch.zeros(1, 3, 96, 96))\n",
    "print('input shape: {} x {} x {}'.format(test_x.shape[1], test_x.shape[2], test_x.shape[3]))\n",
    "test_y = test_net(test_x)\n",
    "print('output shape: {} x {} x {}'.format(test_y.shape[1], test_y.shape[2], test_y.shape[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了 dense block，DenseNet 中还有一个模块叫过渡层（transition block），因为 DenseNet 会不断地对维度进行拼接， 所以当层数很高的时候，输出的通道数就会越来越大，参数和计算量也会越来越大，为了避免这个问题，需要引入过渡层将输出通道降低下来，同时也将输入的长宽减半，这个过渡层可以使用 1 x 1 的卷积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: 3 x 96 x 96\n",
      "output shape: 12 x 48 x 48\n"
     ]
    }
   ],
   "source": [
    "#过渡层\n",
    "def transition(in_channel, out_channel):\n",
    "    trans_layer = nn.Sequential(\n",
    "        nn.BatchNorm2d(in_channel),\n",
    "        nn.ReLU(True),\n",
    "        nn.Conv2d(in_channel, out_channel, 1),\n",
    "        nn.AvgPool2d(2, 2)\n",
    "    )\n",
    "    return trans_layer\n",
    "test_net = transition(3, 12)\n",
    "test_x = Variable(torch.zeros(1, 3, 96, 96))\n",
    "print('input shape: {} x {} x {}'.format(test_x.shape[1], test_x.shape[2], test_x.shape[3]))\n",
    "test_y = test_net(test_x)\n",
    "print('output shape: {} x {} x {}'.format(test_y.shape[1], test_y.shape[2], test_y.shape[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "densenet(\n",
      "  (block1): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1), ceil_mode=False)\n",
      "  )\n",
      "  (block2): Sequential(\n",
      "    (0): dense_block(\n",
      "      (net): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(160, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (4): Sequential(\n",
      "          (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(192, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (5): Sequential(\n",
      "          (0): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(224, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (1): ReLU(inplace)\n",
      "      (2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (3): AvgPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "    )\n",
      "    (2): dense_block(\n",
      "      (net): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(160, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(192, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(224, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (4): Sequential(\n",
      "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (5): Sequential(\n",
      "          (0): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(288, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (6): Sequential(\n",
      "          (0): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(320, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (7): Sequential(\n",
      "          (0): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(352, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (8): Sequential(\n",
      "          (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(384, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (9): Sequential(\n",
      "          (0): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(416, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (10): Sequential(\n",
      "          (0): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(448, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (11): Sequential(\n",
      "          (0): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(480, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (1): ReLU(inplace)\n",
      "      (2): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (3): AvgPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "    )\n",
      "    (4): dense_block(\n",
      "      (net): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(288, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(320, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(352, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (4): Sequential(\n",
      "          (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(384, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (5): Sequential(\n",
      "          (0): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(416, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (6): Sequential(\n",
      "          (0): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(448, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (7): Sequential(\n",
      "          (0): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(480, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (8): Sequential(\n",
      "          (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(512, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (9): Sequential(\n",
      "          (0): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(544, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (10): Sequential(\n",
      "          (0): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(576, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (11): Sequential(\n",
      "          (0): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(608, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (12): Sequential(\n",
      "          (0): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(640, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (13): Sequential(\n",
      "          (0): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(672, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (14): Sequential(\n",
      "          (0): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(704, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (15): Sequential(\n",
      "          (0): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(736, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (16): Sequential(\n",
      "          (0): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(768, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (17): Sequential(\n",
      "          (0): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(800, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (18): Sequential(\n",
      "          (0): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(832, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (19): Sequential(\n",
      "          (0): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(864, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (20): Sequential(\n",
      "          (0): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(896, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (21): Sequential(\n",
      "          (0): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(928, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (22): Sequential(\n",
      "          (0): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(960, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (23): Sequential(\n",
      "          (0): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(992, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (1): ReLU(inplace)\n",
      "      (2): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (3): AvgPool2d(kernel_size=2, stride=2, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "    )\n",
      "    (6): dense_block(\n",
      "      (net): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(512, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(544, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(576, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(608, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (4): Sequential(\n",
      "          (0): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(640, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (5): Sequential(\n",
      "          (0): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(672, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (6): Sequential(\n",
      "          (0): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(704, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (7): Sequential(\n",
      "          (0): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(736, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (8): Sequential(\n",
      "          (0): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(768, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (9): Sequential(\n",
      "          (0): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(800, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (10): Sequential(\n",
      "          (0): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(832, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (11): Sequential(\n",
      "          (0): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(864, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (12): Sequential(\n",
      "          (0): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(896, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (13): Sequential(\n",
      "          (0): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(928, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (14): Sequential(\n",
      "          (0): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(960, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (15): Sequential(\n",
      "          (0): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (1): ReLU(inplace)\n",
      "          (2): Conv2d(992, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu): ReLU(inplace)\n",
      "    (avg_pool): AvgPool2d(kernel_size=3, stride=3, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=1024, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class densenet(nn.Module):\n",
    "    def __init__(self, in_channel, num_classes, growth_rate=32, block_layers=[6, 12, 24, 16]):\n",
    "        super(densenet, self).__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, 64, 7, 2, 3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(3, 2, padding=1)\n",
    "        )\n",
    "        \n",
    "        channels = 64\n",
    "        block = []\n",
    "        for i, layers in enumerate(block_layers):\n",
    "            block.append(dense_block(channels, growth_rate, layers))\n",
    "            channels += layers * growth_rate\n",
    "            if i != len(block_layers) - 1:\n",
    "                block.append(transition(channels, channels // 2)) # 通过 transition 层将大小减半，通道数减半\n",
    "                channels = channels // 2\n",
    "        \n",
    "        self.block2 = nn.Sequential(*block)\n",
    "        self.block2.add_module('bn', nn.BatchNorm2d(channels))\n",
    "        self.block2.add_module('relu', nn.ReLU(True))\n",
    "        self.block2.add_module('avg_pool', nn.AvgPool2d(3))\n",
    "        \n",
    "        self.classifier = nn.Linear(channels, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "print densenet(3,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "test_net = densenet(3, 10)\n",
    "test_x = Variable(torch.zeros(1, 3, 96, 96))\n",
    "test_y = test_net(test_x)\n",
    "print('output: {}'.format(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
