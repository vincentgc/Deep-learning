{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目录\n",
    "* 创建tensor\n",
    "* 查看tensor的基本属性\n",
    "* Squeeze and unsqueeze\n",
    "* Reshape\n",
    "* Variable\n",
    "* 计算导数\n",
    "* 构造网络基本操作\n",
    "    * 各种层\n",
    "    * 训练需要的操作\n",
    "    * 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#载入Pytorch\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numpy_tensor = np.random.randn(10, 20)\n",
    "pytorch_tensor1 = torch.Tensor(numpy_tensor)\n",
    "pytorch_tensor2 = torch.from_numpy(numpy_tensor)\n",
    "#or\n",
    "tensor = torch.randn(10, 20)\n",
    "\n",
    "#放到gpu上\n",
    "gpu_tensor = torch.randn(10, 20).cuda(0) # 将 tensor 放到第一个 GPU 上\n",
    "cpu_tensor = gpu_tensor.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看tensor的基本属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 可以通过下面两种方式得到 tensor 的大小\n",
    "print(pytorch_tensor1.shape)\n",
    "print(pytorch_tensor1.size())\n",
    "# 得到 tensor 的数据类型\n",
    "print(pytorch_tensor1.type())\n",
    "# 得到 tensor 的维度\n",
    "print(pytorch_tensor1.dim())\n",
    "# 得到 tensor 的所有元素个数\n",
    "print(pytorch_tensor1.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-0.1318 -0.1992  0.3353\n",
      "-1.0342  1.1241  1.2663\n",
      " 0.5975 -0.9291 -0.3803\n",
      "-0.5924  0.5201 -0.8944\n",
      "[torch.FloatTensor of size 4x3]\n",
      "\n",
      "\n",
      " 0.0043\n",
      " 1.3562\n",
      "-0.7119\n",
      "-0.9667\n",
      "[torch.FloatTensor of size 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 3)\n",
    "print(x)\n",
    "# 沿着行取最大值\n",
    "max_value, max_idx = torch.max(x, dim=1)\n",
    "# 沿着行对 x 求和\n",
    "sum_x = torch.sum(x, dim=1)\n",
    "print(sum_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squeeze and unsqueeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "torch.Size([1, 4, 3])\n",
      "torch.Size([1, 1, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "# 增加维度或者减少维度\n",
    "print(x.shape)\n",
    "x = x.unsqueeze(0) # 在第一维增加\n",
    "print(x.shape)\n",
    "x = x.unsqueeze(1) # 在第二维增加\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 3])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "x = x.squeeze(0) # 减少第一维\n",
    "print(x.shape)\n",
    "x = x.squeeze() # 将 tensor 中所有的一维全部都去掉\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5])\n",
      "torch.Size([4, 3, 5])\n",
      "torch.Size([5, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 4, 5)\n",
    "print(x.shape)\n",
    "\n",
    "# 使用permute和transpose进行维度交换\n",
    "x = x.permute(1, 0, 2) # permute 可以重新排列 tensor 的维度\n",
    "print(x.shape)\n",
    "\n",
    "x = x.transpose(0, 2)  # transpose 交换 tensor 中的两个维度\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5])\n",
      "torch.Size([12, 5])\n",
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "# 使用 view 对 tensor 进行 reshape\n",
    "x = torch.randn(3, 4, 5)\n",
    "print(x.shape)\n",
    "\n",
    "x = x.view(-1, 5) # -1 表示任意的大小，5 表示第二维变成 5\n",
    "print(x.shape)\n",
    "\n",
    "x = x.view(3, 20) # 重新 reshape 成 (3, 20) 的大小\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 通过下面这种方式导入 Variable\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_tensor = torch.randn(10, 5)\n",
    "y_tensor = torch.randn(5, 10)\n",
    "\n",
    "# 将 tensor 变成 Variable\n",
    "x = Variable(x_tensor, requires_grad=True) # 默认 Variable 是不需要求梯度的，所以我们用这个方式申明需要对其进行求梯度\n",
    "y = Variable(y_tensor, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-8.0746\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "<SumBackward0 object at 0x7fc7381c3690>\n"
     ]
    }
   ],
   "source": [
    "#z = torch.sum(x + y)\n",
    "z = torch.sum(torch.mm(x,y))\n",
    "#打出了 z 中的 tensor 数值，同时通过grad_fn知道了其是通过 Sum 这种方式得到的, grad_fn会记录梯度传递的方式\n",
    "print(z.data)\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.0447\n",
      " 0.9534\n",
      "-2.3593\n",
      " 2.4125\n",
      " 0.5277\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "\n",
      "-1.4006 -0.3198  0.6183 -0.7934  0.7652 -0.8130  0.0484  0.4200 -1.4913  0.1775\n",
      " 0.0587  0.1683  0.7999 -0.5564  0.3898 -0.4051 -1.2345  1.2448 -0.7991  1.8380\n",
      "-0.5075 -1.4926  0.7119  0.3476  2.4557  0.1621  1.9239  0.1158 -0.5363 -0.1629\n",
      "-0.0153 -0.0329 -0.3753 -1.8673  0.5805 -0.6526 -1.3901  0.7534  1.0425  1.5788\n",
      "-1.8903  1.8642  1.1450 -2.0341 -1.6079 -0.5391 -1.6208  1.5804  0.4074  0.1336\n",
      "[torch.FloatTensor of size 5x10]\n",
      "\n",
      "Variable containing:\n",
      "-2.7886  1.5045  3.0177 -0.3784 -2.5616\n",
      "-2.7886  1.5045  3.0177 -0.3784 -2.5616\n",
      "-2.7886  1.5045  3.0177 -0.3784 -2.5616\n",
      "-2.7886  1.5045  3.0177 -0.3784 -2.5616\n",
      "-2.7886  1.5045  3.0177 -0.3784 -2.5616\n",
      "-2.7886  1.5045  3.0177 -0.3784 -2.5616\n",
      "-2.7886  1.5045  3.0177 -0.3784 -2.5616\n",
      "-2.7886  1.5045  3.0177 -0.3784 -2.5616\n",
      "-2.7886  1.5045  3.0177 -0.3784 -2.5616\n",
      "-2.7886  1.5045  3.0177 -0.3784 -2.5616\n",
      "[torch.FloatTensor of size 10x5]\n",
      "\n",
      "Variable containing:\n",
      " 0.0447  0.0447  0.0447  0.0447  0.0447  0.0447  0.0447  0.0447  0.0447  0.0447\n",
      " 0.9534  0.9534  0.9534  0.9534  0.9534  0.9534  0.9534  0.9534  0.9534  0.9534\n",
      "-2.3593 -2.3593 -2.3593 -2.3593 -2.3593 -2.3593 -2.3593 -2.3593 -2.3593 -2.3593\n",
      " 2.4125  2.4125  2.4125  2.4125  2.4125  2.4125  2.4125  2.4125  2.4125  2.4125\n",
      " 0.5277  0.5277  0.5277  0.5277  0.5277  0.5277  0.5277  0.5277  0.5277  0.5277\n",
      "[torch.FloatTensor of size 5x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 求 x 和 y 的梯度\n",
    "z.backward()\n",
    "print torch.sum(x.data,dim=0)\n",
    "print y.data\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算导数例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PowBackward0 object at 0x7fc7391afb90>\n",
      "Variable containing:\n",
      " 12\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#输出结果为标量\n",
    "x = Variable(torch.FloatTensor([2]), requires_grad=True)\n",
    "y = x ** 3\n",
    "print y.grad_fn\n",
    "\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2  2\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      " 0  0\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      " 4  8\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      "  4  12\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#输出结果为向量\n",
    "m = Variable(torch.FloatTensor([[2, 2]]), requires_grad=True) # 构建一个 1 x 2 的矩阵\n",
    "n = Variable(torch.zeros(1, 2)) # 构建一个相同大小的 0 矩阵\n",
    "print(m)\n",
    "print(n)\n",
    "# 通过 m 中的值计算新的 n 中的值\n",
    "n[0, 0] = m[0, 0] ** 2\n",
    "n[0, 1] = m[0, 1] ** 3\n",
    "print(n)\n",
    "n.backward(torch.ones_like(n)) # 将 (w0, w1) 取成 (1, 1)\n",
    "print(m.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多次自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 18\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 8\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 16\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.FloatTensor([3]), requires_grad=True)\n",
    "y = x * 2 + x ** 2 + 3\n",
    "print(y)\n",
    "y.backward(retain_graph=True) # 设置 retain_graph 为 True 来保留计算图\n",
    "print(x.grad)\n",
    "y.backward() # 再做一次自动求导，这次不保留计算图\n",
    "print(x.grad)\n",
    "#这样计算了两次的梯度，两次梯度相加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**小练习**\n",
    "\n",
    "定义\n",
    "\n",
    "$$\n",
    "x = \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "x_0 \\\\\n",
    "x_1\n",
    "\\end{matrix}\n",
    "\\right] = \n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "2 \\\\\n",
    "3\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "k = (k_0,\\ k_1) = (x_0^2 + 3 x_1,\\ 2 x_0 + x_1^2)\n",
    "$$\n",
    "\n",
    "我们希望求得\n",
    "\n",
    "$$\n",
    "j = \\left[\n",
    "\\begin{matrix}\n",
    "\\frac{\\partial k_0}{\\partial x_0} & \\frac{\\partial k_0}{\\partial x_1} \\\\\n",
    "\\frac{\\partial k_1}{\\partial x_0} & \\frac{\\partial k_1}{\\partial x_1}\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "参考答案：\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "\\begin{matrix}\n",
    "4 & 3 \\\\\n",
    "2 & 6 \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 13\n",
      " 13\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "\n",
      " 4  3\n",
      " 2  6\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.FloatTensor([2, 3]), requires_grad=True)\n",
    "k = Variable(torch.zeros(2))\n",
    "\n",
    "k[0] = x[0] ** 2 + 3 * x[1]\n",
    "k[1] = x[1] ** 2 + 2 * x[0]\n",
    "print(k)\n",
    "\n",
    "j = torch.zeros(2, 2)\n",
    "\n",
    "k.backward(torch.FloatTensor([1, 0]), retain_graph=True)\n",
    "j[0] = x.grad.data\n",
    "\n",
    "x.grad.data.zero_() # 归零之前求得的梯度\n",
    "\n",
    "k.backward(torch.FloatTensor([0, 1]))\n",
    "j[1] = x.grad.data\n",
    "\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 构造网络基本操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#数据载入\n",
    "from torch.utils.data import DataLoader\n",
    "# 使用 pytorch 自带的 DataLoader 定义一个数据迭代器\n",
    "#train_set: 自定义一个Dataset: train_set\n",
    "train_data = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "#查看一个batch的数据\n",
    "a, a_label = next(iter(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "# 使用 Sequential 定义 4 层神经网络（全连接层）\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(784, 400),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(400, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 各种层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "#卷积层\n",
    "conv = nn.Conv2d(in_channel, out_channel, kernel, stride, padding)\n",
    "#卷积后的图像：(N - Kernel + 2 * padding) / stride + 1\n",
    "#卷积需要的参数： weights: Kernel *Kernel * in_channel * out_channel  Bias:  out_channel\n",
    "\n",
    "#批标准层\n",
    "bn = nn.BatchNorm2d(out_channel, eps=1e-3),\n",
    "\n",
    "#Relu激活层\n",
    "nn.ReLU(True)\n",
    "\n",
    "#池化层\n",
    "maxpool = nn.MaxPool2d(3, stride=1, padding=1),\n",
    "#池化后的图像：(N - Kernel + 2*padding)/stride + 1\n",
    "#不需要参数\n",
    "\n",
    "#全连接层\n",
    "fc = nn.Linear(400, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练需要的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loss函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#优化函数\n",
    "optimizer = torch.optim.SGD(net.parameters(), 1e-1) # 使用随机梯度下降，学习率 0.1\n",
    "\n",
    "for im, label in train_data:\n",
    "        im = Variable(im)\n",
    "        label = Variable(label)\n",
    "        # 前向传播\n",
    "        out = net(im)\n",
    "        loss = criterion(out, label)\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # 记录误差\n",
    "        train_loss += loss.data[0]\n",
    "        # 计算分类的准确率\n",
    "        _, pred = out.max(1)\n",
    "        num_correct = (pred == label).sum().data[0]\n",
    "        acc = num_correct / im.shape[0]\n",
    "        train_acc += acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "# 定义一个 Sequential 模型\n",
    "net1 = nn.Sequential(\n",
    "    nn.Linear(30, 40),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(40, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 10)\n",
    ")\n",
    "net1[0].weight.data = torch.from_numpy(np.random.uniform(3, 5, size=(40, 30)))\n",
    "\n",
    "#or\n",
    "for layer in net1:\n",
    "    if isinstance(layer, nn.Linear): # 判断是否是线性层\n",
    "        param_shape = layer.weight.shape\n",
    "        layer.weight.data = torch.from_numpy(np.random.normal(0, 0.5, size=param_shape)) \n",
    "        # 定义为均值为 0，方差为 0.5 的正态分布\n",
    "\n",
    "#or\n",
    "from torch.nn import init\n",
    "init.xavier_uniform(net1[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#对于一个网络来说\n",
    "class sim_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(sim_net, self).__init__()\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(30, 40),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.l1[0].weight.data = torch.randn(40, 30) # 直接对某一层初始化\n",
    "        \n",
    "        self.l2 = nn.Sequential(\n",
    "            nn.Linear(40, 50),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.l3 = nn.Sequential(\n",
    "            nn.Linear(50, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x =self.l2(x)\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "\n",
    "for layer in net2.modules():\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        param_shape = layer.weight.shape\n",
    "        layer.weight.data = torch.from_numpy(np.random.normal(0, 0.5, size=param_shape)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
