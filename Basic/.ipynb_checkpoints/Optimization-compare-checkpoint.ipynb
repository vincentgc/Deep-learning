{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 各优化函数的区别\n",
    "source: https://github.com/L1aoXingyu/code-of-learn-deep-learning-with-pytorch/tree/master/chapter3_NN\n",
    "\n",
    "* 梯度\n",
    "* 梯度下降法\n",
    "* 随机梯度下降法\n",
    "    * 学习率下降\n",
    "    * 正则化\n",
    "* 动量法\n",
    "* RMSProp\n",
    "* Adagrad\n",
    "* Adadelta\n",
    "* Adam\n",
    "* 各优化算法对比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度\n",
    "梯度在数学上就是导数，如果是一个多元函数，那么梯度就是偏导数。比如一个函数f(x, y)，那么 f 的梯度就是 \n",
    "\n",
    "$$\n",
    "(\\frac{\\partial f}{\\partial x},\\ \\frac{\\partial f}{\\partial y})\n",
    "$$\n",
    "\n",
    "可以称为 grad f(x, y) 或者 $\\nabla f(x, y)$。具体某一点 $(x_0,\\ y_0)$ 的梯度就是 $\\nabla f(x_0,\\ y_0)$。\n",
    "\n",
    "下面这个图片是 $f(x) = x^2$ 这个函数在 x=1 处的梯度\n",
    "\n",
    "![](https://ws3.sinaimg.cn/large/006tNc79ly1fmarbuh2j3j30ba0b80sy.jpg)\n",
    "梯度有什么意义呢？从几何意义来讲，一个点的梯度值是这个函数变化最快的地方，具体来说，对于函数 f(x, y)，在点 $(x_0, y_0)$ 处，沿着梯度 $\\nabla f(x_0,\\ y_0)$ 的方向，函数增加最快，也就是说沿着梯度的方向，我们能够更快地找到函数的极大值点，或者反过来沿着梯度的反方向，我们能够更快地找到函数的最小值点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降法\n",
    "有了对梯度的理解，我们就能了解梯度下降发的原理了。上面我们需要最小化这个误差，也就是需要找到这个误差的最小值点，那么沿着梯度的反方向我们就能够找到这个最小值点。\n",
    "\n",
    "我们可以来看一个直观的解释。比如我们在一座大山上的某处位置，由于我们不知道怎么下山，于是决定走一步算一步，也就是在每走到一个位置的时候，求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。这样一步步的走下去，一直走到觉得我们已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山峰低处。\n",
    "\n",
    "类比我们的问题，就是沿着梯度的反方向，我们不断改变 w 和 b 的值，最终找到一组最好的 w 和 b 使得误差最小。\n",
    "\n",
    "在更新的时候，我们需要决定每次更新的幅度，比如在下山的例子中，我们需要每次往下走的那一步的长度，这个长度称为学习率，用 $\\eta$ 表示，这个学习率非常重要，不同的学习率都会导致不同的结果，学习率太小会导致下降非常缓慢，学习率太大又会导致跳动非常明显，可以看看下面的例子\n",
    "\n",
    "![](https://ws2.sinaimg.cn/large/006tNc79ly1fmgn23lnzjg30980gogso.gif)\n",
    "\n",
    "可以看到上面的学习率较为合适，而下面的学习率太大，就会导致不断跳动\n",
    "\n",
    "最后我们的更新公式就是\n",
    "\n",
    "$$\n",
    "w := w - \\eta \\frac{\\partial f(w,\\ b)}{\\partial w} \\\\\n",
    "b := b - \\eta \\frac{\\partial f(w,\\ b)}{\\partial b}\n",
    "$$\n",
    "\n",
    "通过不断地迭代更新，最终我们能够找到一组最优的 w 和 b，这就是梯度下降法的原理。\n",
    "\n",
    "最后可以通过这张图形象地说明一下这个方法\n",
    "\n",
    "![](https://ws3.sinaimg.cn/large/006tNc79ly1fmarxsltfqj30gx091gn4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机梯度下降法\n",
    "\n",
    "可以看到 loss 没有 batch 等于 1 震荡那么距离，同时也可以降到一定的程度了，时间上也比之前快了非常多，因为按照 batch 的数据量计算上更快，同时梯度对比于 batch size = 1 的情况也跟接近真实的梯度，所以 batch size 的值越大，梯度也就越稳定，而 batch size 越小，梯度具有越高的随机性，这里 batch size 为 64，可以看到 loss 仍然存在震荡，但这并没有关系，如果 batch size 太大，对于内存的需求就更高，同时也不利于网络跳出局部极小点，所以现在普遍使用基于 batch 的随机梯度下降法，而 batch 的多少基于实际情况进行考虑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimzier = torch.optim.SGD(net.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习率衰减\n",
    "对于基于一阶梯度进行优化的方法而言，开始的时候更新的幅度是比较大的，也就是说开始的学习率可以设置大一点，但是当训练集的 loss 下降到一定程度之后，，使用这个太大的学习率就会导致 loss 一直来回震荡，比如\n",
    "\n",
    "![](https://ws4.sinaimg.cn/large/006tNc79ly1fmrvdlncomj30bf0aywet.jpg)\n",
    "这个时候就需要对学习率进行衰减已达到 loss 的充分下降，而是用学习率衰减的办法能够解决这个矛盾，学习率衰减就是随着训练的进行不断的减小学习率。\n",
    "\n",
    "在 pytorch 中学习率衰减非常方便，使用 `torch.optim.lr_scheduler`，更多的信息可以直接查看[文档](http://pytorch.org/docs/0.3.0/optim.html#how-to-adjust-learning-rate)\n",
    "\n",
    "但是我推荐大家使用下面这种方式来做学习率衰减，更加直观，下面我们直接举例子来说明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, weight_decay=1e-4)#这个也可以看做加了正则项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们定义好了模型和优化器，可以通过 `optimizer.param_groups` 来得到所有的参数组和其对应的属性，参数组是什么意思呢？就是我们可以将模型的参数分成几个组，每个组定义一个学习率，这里比较复杂，一般来讲如果不做特别修改，就只有一个参数组\n",
    "\n",
    "这个参数组是一个字典，里面有很多属性，比如学习率，权重衰减等等，我们可以访问以下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_learning_rate(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "#apply\n",
    "if epoch == 20:\n",
    "    set_learning_rate(optimizer, 0.01) # 20 epoch 次修改学习率为 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则化\n",
    "前面我们讲了数据增强和 dropout，而在实际使用中，现在的网络往往不使用 dropout，而是用另外一个技术，叫正则化。\n",
    "\n",
    "正则化是机器学习中提出来的一种方法，有 L1 和 L2 正则化，目前使用较多的是 L2 正则化，引入正则化相当于在 loss 函数上面加上一项，比如\n",
    "\n",
    "$$\n",
    "f = loss + \\lambda \\sum_{p \\in params} ||p||_2^2\n",
    "$$\n",
    "\n",
    "就是在 loss 的基础上加上了参数的二范数作为一个正则化，我们在训练网络的时候，不仅要最小化 loss 函数，同时还要最小化参数的二范数，也就是说我们会对参数做一些限制，不让它变得太大。\n",
    "如果我们对新的损失函数 f 求导进行梯度下降，就有\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial p_j} = \\frac{\\partial loss}{\\partial p_j} + 2 \\lambda p_j\n",
    "$$\n",
    "\n",
    "那么在更新参数的时候就有\n",
    "\n",
    "$$\n",
    "p_j \\rightarrow p_j - \\eta (\\frac{\\partial loss}{\\partial p_j} + 2 \\lambda p_j) = p_j - \\eta \\frac{\\partial loss}{\\partial p_j} - 2 \\eta \\lambda p_j \n",
    "$$\n",
    "可以看到 $p_j - \\eta \\frac{\\partial loss}{\\partial p_j}$ 和没加正则项要更新的部分一样，而后面的 $2\\eta \\lambda p_j$ 就是正则项的影响，可以看到加完正则项之后会对参数做更大程度的更新，这也被称为权重衰减(weight decay)，在 pytorch 中正则项就是通过这种方式来加入的，比如想在随机梯度下降法中使用正则项，或者说权重衰减，`torch.optim.SGD(net.parameters(), lr=0.1, weight_decay=1e-4)` 就可以了，这个 `weight_decay` 系数就是上面公式中的 $\\lambda$，非常方便\n",
    "\n",
    "注意正则项的系数的大小非常重要，如果太大，会极大的抑制参数的更新，导致欠拟合，如果太小，那么正则项这个部分基本没有贡献，所以选择一个合适的权重衰减系数非常重要，这个需要根据具体的情况去尝试，初步尝试可以使用 `1e-4` 或者 `1e-3` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, weight_decay=1e-4) # 增加正则项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 动量法\n",
    "使用梯度下降法，每次都会朝着目标函数下降最快的方向，这也称为最速下降法。这种更新方法看似非常快，实际上存在一些问题。\n",
    "\n",
    "## 梯度下降法的问题\n",
    "考虑一个二维输入，$[x_1, x_2]$，输出的损失函数 $L: R^2 \\rightarrow R$，下面是这个函数的等高线\n",
    "\n",
    "![](https://ws1.sinaimg.cn/large/006tKfTcly1fmnketw5f4j30az04lq31.jpg)\n",
    "\n",
    "可以想象成一个很扁的漏斗，这样在竖直方向上，梯度就非常大，在水平方向上，梯度就相对较小，所以我们在设置学习率的时候就不能设置太大，为了防止竖直方向上参数更新太过了，这样一个较小的学习率又导致了水平方向上参数在更新的时候太过于缓慢，所以就导致最终收敛起来非常慢。\n",
    "\n",
    "## 动量法\n",
    "动量法的提出就是为了应对这个问题，我们梯度下降法做一个修改如下\n",
    "\n",
    "$$\n",
    "v_i = \\gamma v_{i-1} + \\eta \\nabla L(\\theta)\n",
    "$$\n",
    "$$\n",
    "\\theta_i = \\theta_{i-1} - v_i\n",
    "$$\n",
    "\n",
    "其中 $v_i$ 是当前速度，$\\gamma$ 是动量参数，是一个小于 1的正数，$\\eta$ 是学习率\n",
    "相当于每次在进行参数更新的时候，都会将之前的速度考虑进来，每个参数在各方向上的移动幅度不仅取决于当前的梯度，还取决于过去各个梯度在各个方向上是否一致，如果一个梯度一直沿着当前方向进行更新，那么每次更新的幅度就越来越大，如果一个梯度在一个方向上不断变化，那么其更新幅度就会被衰减，这样我们就可以使用一个较大的学习率，使得收敛更快，同时梯度比较大的方向就会因为动量的关系每次更新的幅度减少，如下图\n",
    "\n",
    "![](https://ws1.sinaimg.cn/large/006tNc79gy1fmo5l53o76j30ak04gjrh.jpg)\n",
    "\n",
    "比如我们的梯度每次都等于 g，而且方向都相同，那么动量法在该方向上使参数加速移动，有下面的公式：\n",
    "\n",
    "$$\n",
    "v_0 = 0\n",
    "$$\n",
    "$$\n",
    "v_1 = \\gamma v_0 + \\eta g = \\eta g\n",
    "$$\n",
    "$$\n",
    "v_2 = \\gamma v_1 + \\eta g = (1 + \\gamma) \\eta g\n",
    "$$\n",
    "$$\n",
    "v_3 = \\gamma v_2 + \\eta g = (1 + \\gamma + \\gamma^2) \\eta g\n",
    "$$\n",
    "$$\n",
    "\\cdots\n",
    "$$\n",
    "$$\n",
    "v_{+ \\infty} = (1 + \\gamma + \\gamma^2 + \\gamma^3 + \\cdots) \\eta g = \\frac{1}{1 - \\gamma} \\eta g\n",
    "$$\n",
    "\n",
    "如果我们把 $\\gamma$ 定为 0.9，那么更新幅度的峰值就是原本梯度乘学习率的 10 倍。\n",
    "\n",
    "本质上说，动量法就仿佛我们从高坡上推一个球，小球在向下滚动的过程中积累了动量，在途中也会变得越来越快，最后会达到一个峰值，对应于我们的算法中就是，动量项会沿着梯度指向方向相同的方向不断增大，对于梯度方向改变的方向逐渐减小，得到了更快的收敛速度以及更小的震荡。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-2, momentum=0.9) # 加动量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp\n",
    "RMSprop 是由 Geoff Hinton 在他 Coursera 课程中提出的一种适应性学习率方法，至今仍未被公开发表。前面我们提到了 Adagrad 算法有一个问题，就是学习率分母上的变量 s 不断被累加增大，最后会导致学习率除以一个比较大的数之后变得非常小，这不利于我们找到最后的最优解，所以 RMSProp 的提出就是为了解决这个问题。\n",
    "\n",
    "## RMSProp 算法\n",
    "RMSProp 仍然会使用梯度的平方量，不同于 Adagrad，其会使用一个指数加权移动平均来计算这个 s，也就是\n",
    "\n",
    "$$\n",
    "s_i = \\alpha s_{i-1} + (1 - \\alpha) \\ g^2\n",
    "$$\n",
    "\n",
    "这里 g 表示当前求出的参数梯度，然后最终更新和 Adagrad 是一样的，学习率变成了\n",
    "\n",
    "$$\n",
    "\\frac{\\eta}{\\sqrt{s + \\epsilon}}\n",
    "$$\n",
    "\n",
    "这里 $\\alpha$ 是一个移动平均的系数，也是因为这个系数，导致了 RMSProp 和 Adagrad 不同的地方，这个系数使得 RMSProp 更新到后期累加的梯度平方较小，从而保证 s 不会太大，也就使得模型后期依然能够找到比较优的结果\n",
    "\n",
    "实现上和 Adagrad 非常像"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adagrad\n",
    "这个优化算法被称为自适应学习率优化算法，之前我们讲的随机梯度下降以及动量法对所有的参数都使用的固定的学习率进行参数更新，但是不同的参数梯度可能不一样，所以需要不同的学习率才能比较好的进行训练，但是这个事情又不能很好地被人为操作，所以 Adagrad 便能够帮助我们做这件事。\n",
    "\n",
    "## Adagrad 算法\n",
    "Adagrad 的想法非常简答，在每次使用一个 batch size 的数据进行参数更新的时候，我们需要计算所有参数的梯度，那么其想法就是对于每个参数，初始化一个变量 s 为 0，然后每次将该参数的梯度平方求和累加到这个变量 s 上，然后在更新这个参数的时候，学习率就变为\n",
    "\n",
    "$$\n",
    "\\frac{\\eta}{\\sqrt{s + \\epsilon}}\n",
    "$$\n",
    "\n",
    "这里的 $\\epsilon$ 是为了数值稳定性而加上的，因为有可能 s 的值为 0，那么 0 出现在分母就会出现无穷大的情况，通常 $\\epsilon$ 取 $10^{-10}$，这样不同的参数由于梯度不同，他们对应的 s 大小也就不同，所以上面的公式得到的学习率也就不同，这也就实现了自适应的学习率。\n",
    "\n",
    "Adagrad 的核心想法就是，如果一个参数的梯度一直都非常大，那么其对应的学习率就变小一点，防止震荡，而一个参数的梯度一直都非常小，那么这个参数的学习率就变大一点，使得其能够更快地更新\n",
    "\n",
    "Adagrad 也有一些问题，因为 s 不断累加梯度的平方，所以会越来越大，导致学习率在后期会变得较小，导致收敛乏力的情况，可能无法收敛到表较好的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adagrad(net.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp\n",
    "RMSprop 是由 Geoff Hinton 在他 Coursera 课程中提出的一种适应性学习率方法，至今仍未被公开发表。前面我们提到了 Adagrad 算法有一个问题，就是学习率分母上的变量 s 不断被累加增大，最后会导致学习率除以一个比较大的数之后变得非常小，这不利于我们找到最后的最优解，所以 RMSProp 的提出就是为了解决这个问题。\n",
    "\n",
    "## RMSProp 算法\n",
    "RMSProp 仍然会使用梯度的平方量，不同于 Adagrad，其会使用一个指数加权移动平均来计算这个 s，也就是\n",
    "\n",
    "$$\n",
    "s_i = \\alpha s_{i-1} + (1 - \\alpha) \\ g^2\n",
    "$$\n",
    "\n",
    "这里 g 表示当前求出的参数梯度，然后最终更新和 Adagrad 是一样的，学习率变成了\n",
    "\n",
    "$$\n",
    "\\frac{\\eta}{\\sqrt{s + \\epsilon}}\n",
    "$$\n",
    "\n",
    "这里 $\\alpha$ 是一个移动平均的系数，也是因为这个系数，导致了 RMSProp 和 Adagrad 不同的地方，这个系数使得 RMSProp 更新到后期累加的梯度平方较小，从而保证 s 不会太大，也就使得模型后期依然能够找到比较优的结果\n",
    "\n",
    "实现上和 Adagrad 非常像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.RMSprop(net.parameters(), lr=1e-3, alpha=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adadelta\n",
    "Adadelta 算是 Adagrad 法的延伸，它跟 RMSProp 一样，都是为了解决 Adagrad 中学习率不断减小的问题，RMSProp 是通过移动加权平均的方式，而 Adadelta 也是一种方法，有趣的是，它并不需要学习率这个参数。\n",
    "\n",
    "## Adadelta 法\n",
    "Adadelta 跟 RMSProp 一样，先使用移动平均来计算 s\n",
    "\n",
    "$$\n",
    "s = \\rho s + (1 - \\rho) g^2\n",
    "$$\n",
    "\n",
    "这里 $\\rho$ 和 RMSProp 中的 $\\alpha$ 都是移动平均系数，g 是参数的梯度，然后我们会计算需要更新的参数的变化量\n",
    "\n",
    "$$\n",
    "g' = \\frac{\\sqrt{\\Delta \\theta + \\epsilon}}{\\sqrt{s + \\epsilon}} g\n",
    "$$\n",
    "\n",
    "$\\Delta \\theta$ 初始为 0 张量，每一步做如下的指数加权移动平均更新\n",
    "\n",
    "$$\n",
    "\\Delta \\theta = \\rho \\Delta \\theta + (1 - \\rho) g'^2\n",
    "$$\n",
    "\n",
    "最后参数更新如下\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - g'\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adadelta(net.parameters(), rho=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam\n",
    "Adam 是一个结合了动量法和 RMSProp 的优化算法，其结合了两者的优点。\n",
    "\n",
    "## Adam 算法\n",
    "Adam 算法会使用一个动量变量 v 和一个 RMSProp 中的梯度元素平方的移动指数加权平均 s，首先将他们全部初始化为 0，然后在每次迭代中，计算他们的移动加权平均进行更新\n",
    "\n",
    "$$\n",
    "v = \\beta_1 v + (1 - \\beta_1) g \\\\\n",
    "s = \\beta_2 s + (1 - \\beta_2) g^2\n",
    "$$\n",
    "\n",
    "在 adam 算法里，为了减轻 v 和 s 被初始化为 0 的初期对计算指数加权移动平均的影响，每次 v 和 s 都做下面的修正\n",
    "\n",
    "$$\n",
    "\\hat{v} = \\frac{v}{1 - \\beta_1^t} \\\\\n",
    "\\hat{s} = \\frac{s}{1 - \\beta_2^t}\n",
    "$$\n",
    "\n",
    "这里 t 是迭代次数，可以看到，当 $0 \\leq \\beta_1, \\beta_2 \\leq 1$ 的时候，迭代到后期 t 比较大，那么 $\\beta_1^t$ 和 $\\beta_2^t$ 就几乎为 0，就不会对 v 和 s 有任何影响了，算法作者建议$\\beta_1 = 0.9$, $\\beta_2 = 0.999$。\n",
    "\n",
    "最后使用修正之后的 $\\hat{v}$ 和 $\\hat{s}$ 进行学习率的重新计算\n",
    "\n",
    "$$\n",
    "g' = \\frac{\\eta \\hat{v}}{\\sqrt{\\hat{s} + \\epsilon}}\n",
    "$$\n",
    "\n",
    "这里 $\\eta$ 是学习率，$epsilon$ 仍然是为了数值稳定性而添加的常数，最后参数更新有\n",
    "\n",
    "$$\n",
    "\\theta_i = \\theta_{i-1} - g'\n",
    "$$\n",
    "\n",
    "使用 adam 算法 loss 能够更快更好地收敛，但是一定要小心学习率的设定，使用自适应的算法一般需要更小的学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 各优化算法对比\n",
    "\n",
    "![](https://raw.githubusercontent.com/cs231n/cs231n.github.io/master/assets/nn3/opt1.gif)\n",
    "\n",
    "![](https://raw.githubusercontent.com/cs231n/cs231n.github.io/master/assets/nn3/opt2.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
